---
title: "Qwen Code Integration"
description: "Configure Qwen Code with Adaptive's intelligent routing for optimal model selection and 60-90% cost savings"
icon: "/images/logos/qwen.svg"
---

Configure Qwen Code to use Adaptive's intelligent routing infrastructure that automatically selects the optimal AI model for each coding task.

<Tip>
  **Intelligent model selection** with automatic routing. Works as a drop-in
  replacement for Qwen Code's API backend, providing access to multiple AI
  providers (Claude, GPT-4, etc.) with cost optimization and intelligent routing
  built-in.
</Tip>

## Benefits of Using Qwen Code with Adaptive

When you integrate Qwen Code with Adaptive, you unlock powerful capabilities:

- **Multi-Provider Access**: Access Claude, GPT-4, Gemini, and other providers through a single interface
- **Intelligent Model Selection**: Adaptive automatically routes requests to the optimal model based on task complexity, language, and context
- **Cost Optimization**: Save 60-90% on API costs through intelligent routing and model selection
- **Higher Reliability**: Automatic fallbacks across providers ensure consistent responses
- **Enhanced Performance**: Load balancing and circuit breakers for optimal throughput
- **Usage Analytics**: Monitor model usage, costs, and performance in real-time

## Get Your Adaptive API Key

[Sign up here](https://www.llmadaptive.uk/sign-up?redirect_url=/api-platform/orgs) to create an account and generate your API key.

## Quick Setup

<Steps>
  <Step title="Run Automated Installer" icon="rocket">
    ```bash
    curl -fsSL https://raw.githubusercontent.com/Egham-7/adaptive/main/scripts/installers/qwen-code.sh | bash
    ```

    The installer will automatically:
    - Install Qwen Code if not present (via npm)
    - Configure OpenAI-compatible environment variables for Adaptive
    - Add configuration to your shell profile (~/.bashrc, ~/.zshrc, etc.)
    - Verify the installation
  </Step>
  <Step title="Verify Configuration" icon="check">
    ```bash
    qwen --version
    echo $OPENAI_API_KEY
    echo $OPENAI_BASE_URL
    ```
  </Step>
  <Step title="Start Using" icon="play">
    ```bash
    qwen
    # Or start with a prompt
    qwen "help me refactor this function"
    ```

    Adaptive will automatically route your request to the optimal model for coding tasks.
  </Step>
</Steps>

## Manual Installation

If you prefer to set up Qwen Code manually or need more control over the installation process:

### Step 1: Install Qwen Code

```bash
npm install -g @qwen-code/qwen-code@latest
```

<Note>
  Qwen Code requires Node.js 20 or higher. Check your version with `node --version`.
</Note>

### Step 2: Configure Environment Variables

Qwen Code uses OpenAI-compatible API configuration:

<CodeGroup>
```bash Bash/Zsh (~/.bashrc or ~/.zshrc)
# Qwen Code with Adaptive LLM API Configuration
export OPENAI_API_KEY="your-adaptive-api-key-here"  # qwen-code
export OPENAI_BASE_URL="https://api.llmadaptive.uk/v1"  # qwen-code
export OPENAI_MODEL="intelligent-routing"  # qwen-code - for automatic model selection
```

```fish Fish Shell (~/.config/fish/config.fish)
# Qwen Code with Adaptive LLM API Configuration
set -x OPENAI_API_KEY "your-adaptive-api-key-here"  # qwen-code
set -x OPENAI_BASE_URL "https://api.llmadaptive.uk/v1"  # qwen-code
set -x OPENAI_MODEL "intelligent-routing"  # qwen-code - for automatic model selection
```
</CodeGroup>

### Step 3: Apply Configuration

```bash
# For Bash/Zsh
source ~/.bashrc  # or ~/.zshrc

# For Fish
source ~/.config/fish/config.fish

# Or restart your terminal
```

### Step 4: Verify Installation

```bash
qwen --version
qwen "test connection"
```

## Alternative Setup Methods

<CodeGroup>
```bash Environment Variable Method
export ADAPTIVE_API_KEY='your-api-key-here'
curl -fsSL https://raw.githubusercontent.com/Egham-7/adaptive/main/scripts/installers/qwen-code.sh | bash
# The installer will automatically configure your shell
```

```bash Interactive Installation
curl -o qwen-code.sh https://raw.githubusercontent.com/Egham-7/adaptive/main/scripts/installers/qwen-code.sh
chmod +x qwen-code.sh
./qwen-code.sh
# Follow the interactive prompts
```

```bash Custom Model Configuration
export ADAPTIVE_API_KEY='your-api-key-here'
export ADAPTIVE_MODEL='qwen:qwen-plus'  # Optional: specify default model
curl -fsSL https://raw.githubusercontent.com/Egham-7/adaptive/main/scripts/installers/qwen-code.sh | bash
```
</CodeGroup>

## Advanced Configuration

### Model Selection with Adaptive

Configure which provider and model to use by default:

<CodeGroup>
```bash Intelligent Routing (Recommended)
# Let Adaptive choose the optimal model for each task
export OPENAI_MODEL='intelligent-routing'
qwen "complex algorithm optimization"
```

```bash Qwen Models
# Use Qwen-Coder models specifically
export OPENAI_MODEL='qwen:qwen-plus'
qwen "explain this Python code"

# Or use Qwen Turbo for faster responses
export OPENAI_MODEL='qwen:qwen-turbo'
qwen "what is a closure?"
```

```bash Anthropic Claude
# Use Claude for complex reasoning
export OPENAI_MODEL='anthropic:claude-sonnet-4-20250514'
qwen "refactor this codebase"
```

```bash OpenAI Models
# Use GPT-4 for specific tasks
export OPENAI_MODEL='openai:gpt-4'
qwen "write comprehensive tests"
```
</CodeGroup>

### Intelligent Routing

When `OPENAI_MODEL` is set to `"intelligent-routing"` or empty, Adaptive intelligently selects the best model for each task based on:

- **Task Complexity**: Analyzes prompt complexity to select the optimal model
- **Language & Framework**: Matches model strengths to programming languages
- **Code Context**: Understands codebase size and complexity
- **Performance Requirements**: Balances speed and quality
- **Cost Optimization**: Automatically minimizes costs while maintaining quality
- **Provider Availability**: Automatic fallback if a provider is unavailable

### Available Model Providers

| Provider      | Models                | Best For                         | Speed  | Cost   |
| ------------- | --------------------- | -------------------------------- | ------ | ------ |
| **Qwen**      | qwen-plus, qwen-turbo | Code generation, Asian languages | Fast   | Low    |
| **Anthropic** | Claude Sonnet 4       | Complex reasoning, refactoring   | Medium | Medium |
| **OpenAI**    | GPT-4, GPT-4 Turbo    | General coding, documentation    | Medium | Higher |
| **Google**    | Gemini Pro, Flash     | Code review, analysis            | Fast   | Medium |
| **DeepSeek**  | deepseek-coder        | Code completion, debugging       | Fast   | Low    |

## Usage Examples

### Code Understanding & Editing

<CodeGroup>
```bash Explore Codebases
cd your-project/
qwen

# Architecture analysis
> Describe the main pieces of this system's architecture
> What are the key dependencies and how do they interact?
> Find all API endpoints and their authentication methods
```

```bash Code Development
# Refactoring
> Refactor this function to improve readability and performance
> Convert this class to use dependency injection
> Split this large module into smaller, focused components

# Code generation
> Create a REST API endpoint for user management
> Generate unit tests for the authentication module
> Add error handling to all database operations
```

```bash Debugging & Analysis
# Performance analysis
> Identify performance bottlenecks in this React component
> Find all N+1 query problems in the codebase

# Security audit
> Check for potential SQL injection vulnerabilities
> Find all hardcoded credentials or API keys
```
</CodeGroup>

### Workflow Automation

<CodeGroup>
```bash Git Automation
# Analyze git commits from the last 7 days, grouped by feature
> git log --since="7 days ago" --pretty=format:"%h - %s" --graph

# Create a changelog from recent commits
> Generate a CHANGELOG.md from commits since last release

# Find all TODO comments and create GitHub issues
> Find all TODO comments and create corresponding issues
```

```bash File Operations
# Convert all images in this directory to PNG format
> Convert all .jpg files to .png in the current directory

# Rename all test files to follow the *.test.ts pattern
> Rename all test files to use .test.ts extension

# Find and remove all console.log statements
> Find and remove console.log statements from all JavaScript files
```
</CodeGroup>

### Session Management

Control your token usage with configurable session limits:

<CodeGroup>
```bash Configure Session Limit
# Create or edit .qwen/settings.json in your home directory
{
  "sessionTokenLimit": 32000
}
```

```bash Session Commands
# Compress conversation history to continue within token limits
/compress

# Clear all conversation history and start fresh
/clear

# Check current token usage and limits
/stats
```
</CodeGroup>

<Note>
  Session token limit applies to a single conversation, not cumulative API calls.
</Note>

### Vision Model Support

Qwen Code includes automatic vision model detection for image analysis:

<CodeGroup>
```bash Auto Vision Detection
# Include images in your queries
qwen "Analyze this UI screenshot and suggest improvements" --image screenshot.png

# Vision model will automatically switch when images are detected

# Configure behavior in .qwen/settings.json
{
  "experimental": {
    "vlmSwitchMode": "once" // "once", "session", "persist", or omit for interactive
  }
}
```

```bash Vision Model Modes
# Switch once per query, then revert
qwen --vlm-switch-mode once "Analyze this diagram"

# Switch for entire session
qwen --vlm-switch-mode session

# Never switch automatically
qwen --vlm-switch-mode persist

# Disable vision support completely
{
  "experimental": {
    "visionModelPreview": false
  }
}
```
</CodeGroup>

## Integration with Adaptive Features

### Cost Optimization

<AccordionGroup>
<Accordion title="Intelligent Model Routing" icon="route">
Adaptive automatically routes your requests to the most cost-effective model that meets quality requirements:

**Before Adaptive**: Fixed model costs
- GPT-4: $0.03/1K tokens (input) + $0.06/1K tokens (output)
- Claude Sonnet: $0.003/1K tokens (input) + $0.015/1K tokens (output)

**With Adaptive**: Intelligent routing saves 60-90%
- Simple queries → Qwen Turbo: $0.0008/1K tokens
- Moderate tasks → Qwen Plus: $0.002/1K tokens
- Complex reasoning → Claude Sonnet: $0.003/1K tokens

**Example Savings**:
- 1M tokens/month without Adaptive: ~$45
- 1M tokens/month with Adaptive: ~$12
- **Monthly savings: $33 (73% reduction)**
</Accordion>

<Accordion title="Semantic Caching" icon="database">
Adaptive caches similar requests to reduce API calls:

**How it works**:
- Semantic similarity detection for code queries
- Automatic cache hits for similar questions
- Configurable cache TTL and similarity threshold

**Cost Impact**:
- Cache hit rate: 30-40% for typical dev workflows
- Additional savings: 20-30% on top of intelligent routing
- Zero latency for cached responses
</Accordion>

<Accordion title="Load Balancing" icon="scale-balanced">
Distribute requests across providers for optimal performance:

**Benefits**:
- Higher rate limits through multi-provider distribution
- Automatic failover if one provider is down
- Geographic routing for lower latency
- Cost-optimized provider selection

**Performance Impact**:
- 99.9% uptime with automatic failover
- 50% higher effective rate limits
- 20-30% latency reduction with geographic routing
</Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
<Accordion title="Installation Issues" icon="wrench">
**Problem**: Qwen Code installation fails

**Solutions**:
- Ensure Node.js 20+ is installed: `node --version`
- Install Node.js if needed:

  ```bash
  # Using nvm (recommended)
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  nvm install 22
  ```

- Check npm permissions: `npm config get prefix`
- Try with sudo (not recommended): `sudo npm install -g @qwen-code/qwen-code`
- Clear npm cache: `npm cache clean --force`
</Accordion>

<Accordion title="Authentication Errors" icon="key">
**Problem**: "Unauthorized" or "Invalid API key" errors

**Solutions**:
1. Verify your API key at [llmadaptive.uk/dashboard](https://www.llmadaptive.uk/dashboard)
2. Check environment variables are set:

   ```bash
   echo $OPENAI_API_KEY
   echo $OPENAI_BASE_URL
   echo $OPENAI_MODEL
   ```

3. Ensure variables are exported in your shell config:

   ```bash
   # Bash/Zsh
   source ~/.bashrc  # or ~/.zshrc

   # Fish
   source ~/.config/fish/config.fish
   ```

4. Restart your terminal if changes were made to shell config
5. Verify the base URL is correct: `https://api.llmadaptive.uk/v1`
6. Check for the `# qwen-code` comment to ensure correct environment variables
</Accordion>

<Accordion title="Connection Errors" icon="network-wired">
**Problem**: Cannot connect to Adaptive API

**Solutions**:
- Check internet connectivity
- Verify base URL is correct: `echo $OPENAI_BASE_URL`
- Test API directly:

  ```bash
  curl -X POST https://api.llmadaptive.uk/v1/chat/completions \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "model": "intelligent-routing",
      "messages": [{"role": "user", "content": "Hello"}]
    }'
  ```

- Check if your network/firewall blocks the API endpoint
- Try using a different network or VPN
</Accordion>

<Accordion title="Model Routing Issues" icon="route">
**Problem**: Requests not routing to expected models

**Solutions**:
1. Check current model configuration:

   ```bash
   echo $OPENAI_MODEL
   ```

2. Use intelligent routing for automatic selection:

   ```bash
   export OPENAI_MODEL='intelligent-routing'
   ```

3. Verify provider:model format:

   ```bash
   export OPENAI_MODEL='qwen:qwen-plus'  # Correct
   export OPENAI_MODEL='qwen-plus'       # Incorrect
   ```

4. Check Adaptive dashboard for routing logs and model availability
5. Review model names match supported providers
</Accordion>

<Accordion title="Performance Issues" icon="gauge-high">
**Problem**: Slow response times or timeouts

**Solutions**:
- Check Adaptive dashboard for provider status
- Verify rate limits aren't exceeded
- Use faster models for simple tasks:

  ```bash
  export OPENAI_MODEL='qwen:qwen-turbo'
  ```

- Enable semantic caching for repeated queries
- Check your internet connection speed
- Review model selection—Qwen Turbo and Flash models are faster
- Consider load balancing configuration in Adaptive dashboard
</Accordion>

<Accordion title="Session Token Limits" icon="gauge">
**Problem**: Hitting token limits in long sessions

**Solutions**:
- Configure higher session limits in `.qwen/settings.json`:

  ```json
  {
    "sessionTokenLimit": 64000
  }
  ```

- Use session compression to reduce token usage:

  ```bash
  /compress
  ```

- Clear conversation history and start fresh:

  ```bash
  /clear
  ```

- Monitor token usage:

  ```bash
  /stats
  ```

- Break large tasks into smaller sessions
</Accordion>
</AccordionGroup>

## Uninstallation

If you need to remove Qwen Code or revert configuration:

<Steps>
  <Step title="Remove Qwen Code">
    ```bash
    npm uninstall -g @qwen-code/qwen-code
    ```
  </Step>
  <Step title="Remove Environment Variables">
    Edit your shell config file and remove these lines:

    ```bash
    # Qwen Code with Adaptive LLM API Configuration
    export OPENAI_API_KEY="..." # qwen-code
    export OPENAI_BASE_URL="..." # qwen-code
    export OPENAI_MODEL="..." # qwen-code
    ```
  </Step>
  <Step title="Reload Shell Configuration">
    ```bash
    source ~/.bashrc # or ~/.zshrc or ~/.config/fish/config.fish
    ```
  </Step>
</Steps>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Monitor Usage & Savings"
    href="https://www.llmadaptive.uk/dashboard"
    icon="chart-line"
  >
    Track your cost savings and usage analytics in real-time
  </Card>
  <Card
    title="API Documentation"
    href="/api-reference/chat-completions"
    icon="book"
  >
    Learn about Adaptive's API capabilities and advanced features
  </Card>
  <Card
    title="More CLI Tools"
    href="/developer-tools/claude-code"
    icon="terminal"
  >
    Explore other CLI tools with Adaptive integration
  </Card>
  <Card
    title="Advanced Routing"
    href="/features/intelligent-routing"
    icon="route"
  >
    Learn about intelligent model routing and load balancing
  </Card>
</CardGroup>

<CardGroup cols={2}>
  <Card
    title="Monitor Usage & Savings"
    href="https://www.llmadaptive.uk/dashboard"
    icon="chart-line"
  >
    Track your cost savings and usage analytics in real-time
  </Card>
  <Card
    title="API Documentation"
    href="/api-reference/chat-completions"
    icon="book"
  >
    Learn about Adaptive's API capabilities and advanced features
  </Card>
  <Card
    title="More CLI Tools"
    href="/developer-tools/claude-code"
    icon="terminal"
  >
    Explore other CLI tools with Adaptive integration
  </Card>
  <Card
    title="Advanced Routing"
    href="/features/intelligent-routing"
    icon="route"
  >
    Learn about intelligent model routing and load balancing
  </Card>
</CardGroup>

---

<Note>
  **Was this page helpful?** Contact us at
  [info@llmadaptive.uk](mailto:info@llmadaptive.uk) for feedback or assistance
  with your Qwen Code integration.
</Note>