---
title: "Model Selection Example"
description: "Intelligent model selection without inference - perfect for routing decisions and cost planning"
icon: "route"
---

# Model Selection Example

Use Adaptive's intelligent model selection to get routing decisions without running inference. Perfect for testing strategies, cost planning, and integrating with your own provider accounts.

<Info>
This example demonstrates the `/select-model` endpoint which returns optimal model choices based on your prompt and criteria without actually running inference.
</Info>

## Quick Start

The select-model endpoint helps you choose the best model from your available options based on prompt complexity, cost optimization, and task requirements.

<RequestExample>

```typescript Basic Selection
const response = await fetch("https://llmadaptive.uk/api/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": "your-adaptive-api-key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      { provider: "openai" },
      { provider: "anthropic" },
      { provider: "google" },
    ],
    prompt: "Hello, how are you today?",
  }),
});

const result = await response.json();
console.log(`Selected: ${result.provider}/${result.model}`);
```

```python Python
import requests

response = requests.post(
    "https://llmadaptive.uk/api/v1/select-model",
    headers={
        "X-Stainless-API-Key": "your-adaptive-api-key",
        "Content-Type": "application/json"
    },
    json={
        "models": [
            {"provider": "openai"},
            {"provider": "anthropic"},
            {"provider": "google"}
        ],
        "prompt": "Hello, how are you today?"
    }
)

result = response.json()
print(f"Selected: {result['provider']}/{result['model']}")
```

```bash cURL
curl -X POST "https://llmadaptive.uk/api/v1/select-model" \
  -H "X-Stainless-API-Key: your-adaptive-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "models": [
      {"provider": "openai"},
      {"provider": "anthropic"},
      {"provider": "google"}
    ],
    "prompt": "Hello, how are you today?"
  }'
```

</RequestExample>

<ResponseExample>

```json Response
{
  "provider": "openai",
  "model": "gpt-4o-mini",
  "alternatives": [
    {
      "provider": "openai",
      "model": "gpt-4o"
    }
  ]
}
```

</ResponseExample>

## Cost vs Performance Optimization

Control the balance between cost and performance using the `cost_bias` parameter.

<Callout icon="chart-line" color="#10B981">
**Cost Bias Scale**: 0.0 = cheapest models, 1.0 = best performance models
</Callout>

<RequestExample>

```typescript Performance-Focused (cost_bias=0.9)
const performanceResponse = await fetch("https://llmadaptive.uk/api/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": "your-adaptive-api-key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      { provider: "openai", model_name: "gpt-4o-mini" },
      { provider: "openai", model_name: "gpt-4o" },
      { provider: "anthropic", model_name: "claude-3-5-sonnet-20241022" },
    ],
    prompt: "Write a comprehensive analysis of market trends",
    cost_bias: 0.9, // Prioritize performance
  }),
});
```

```typescript Cost-Focused (cost_bias=0.1)
const costResponse = await fetch("https://llmadaptive.uk/api/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": "your-adaptive-api-key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      { provider: "openai", model_name: "gpt-4o-mini" },
      { provider: "openai", model_name: "gpt-4o" },
      { provider: "anthropic", model_name: "claude-3-5-sonnet-20241022" },
    ],
    prompt: "Hello, how are you?",
    cost_bias: 0.1, // Prioritize cost savings
  }),
});
```

</RequestExample>

<ResponseExample>

```json Performance-Focused Result
{
  "provider": "anthropic",
  "model": "claude-3-5-sonnet-20241022"
}
```

```json Cost-Focused Result
{
  "provider": "openai",
  "model": "gpt-4o-mini"
}
```

</ResponseExample>

## Function Calling Support

When tools are provided, Adaptive automatically prioritizes models that support function calling.

<RequestExample>

```typescript Function Calling
const functionResponse = await fetch("https://llmadaptive.uk/api/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": "your-adaptive-api-key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      { provider: "openai", model_name: "gpt-4o-mini" },
      { provider: "anthropic", model_name: "claude-3-haiku-20240307" },
      { provider: "google", model_name: "gemini-1.5-flash" },
    ],
    prompt: "What's the current weather in San Francisco?",
    tools: [
      {
        type: "function",
        function: {
          name: "get_weather",
          description: "Get current weather for a location",
          parameters: {
            type: "object",
            properties: {
              location: {
                type: "string",
                description: "The city name to get weather for",
              },
            },
            required: ["location"],
          },
        },
      },
    ],
  }),
});
```

</RequestExample>

<ResponseExample>

```json Function Calling Result
{
  "provider": "google",
  "model": "gemini-1.5-flash"
}
```

</ResponseExample>

## Custom Models

Mix known cloud models with your custom local or fine-tuned models for hybrid deployments.

<RequestExample>

```typescript Custom Model
const customResponse = await fetch("https://llmadaptive.uk/api/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": "your-adaptive-api-key",
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      { provider: "openai", model_name: "gpt-4o-mini" }, // Known model
      {
        // Custom model specification
        provider: "local",
        model_name: "my-custom-llama-fine-tune",
        cost_per_1m_input_tokens: 0.0, // Free since it's local
        cost_per_1m_output_tokens: 0.0,
        max_context_tokens: 4096,
        supports_tool_calling: false,
        complexity: "medium",
        task_type: "Text Generation",
      },
    ],
    prompt: "Hello, how are you?",
    cost_bias: 0.1, // Prioritize cost savings to select the free local model
  }),
});
```

</RequestExample>

<ResponseExample>

```json Custom Model Result
{
  "provider": "local",
  "model": "my-custom-llama-fine-tune"
}
```

</ResponseExample>

## Use Cases

<Columns cols={2}>
  <Card title="Cost Planning" icon="calculator" color="#10B981">
    Test different routing strategies to estimate costs before implementation
  </Card>
  <Card title="A/B Testing" icon="split" color="#3B82F6">
    Compare model selection across different cost_bias values and prompts
  </Card>
  <Card title="Integration Testing" icon="plug" color="#8B5CF6">
    Validate routing decisions before connecting to your provider accounts
  </Card>
  <Card title="Hybrid Deployments" icon="server" color="#F59E0B">
    Mix cloud and on-premise models for optimal cost and latency
  </Card>
</Columns>

## Complete Example

Here's a full example that demonstrates all features:

<RequestExample>

```typescript Complete Example
async function demonstrateModelSelection() {
  const apiKey = process.env.ADAPTIVE_API_KEY;
  const baseURL = "https://llmadaptive.uk/api/v1";

  // Example 1: Basic selection
  const basicResponse = await fetch(`${baseURL}/select-model`, {
    method: "POST",
    headers: {
      "X-Stainless-API-Key": apiKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      models: [
        { provider: "openai" },
        { provider: "anthropic" },
        { provider: "google" },
      ],
      prompt: "Hello, how are you today?",
    }),
  });

  const basicResult = await basicResponse.json();
  console.log(`Basic: ${basicResult.provider}/${basicResult.model}`);

  // Example 2: Performance-focused
  const performanceResponse = await fetch(`${baseURL}/select-model`, {
    method: "POST",
    headers: {
      "X-Stainless-API-Key": apiKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      models: [
        { provider: "openai", model_name: "gpt-4o-mini" },
        { provider: "anthropic", model_name: "claude-3-5-sonnet-20241022" },
      ],
      prompt: "Write a comprehensive analysis",
      cost_bias: 0.9,
    }),
  });

  const performanceResult = await performanceResponse.json();
  console.log(`Performance: ${performanceResult.provider}/${performanceResult.model}`);

  // Example 3: Cost-focused
  const costResponse = await fetch(`${baseURL}/select-model`, {
    method: "POST",
    headers: {
      "X-Stainless-API-Key": apiKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      models: [
        { provider: "openai", model_name: "gpt-4o-mini" },
        { provider: "anthropic", model_name: "claude-3-5-sonnet-20241022" },
      ],
      prompt: "Hello",
      cost_bias: 0.1,
    }),
  });

  const costResult = await costResponse.json();
  console.log(`Cost-focused: ${costResult.provider}/${costResult.model}`);
}

demonstrateModelSelection();
```

</RequestExample>

<ResponseExample>

```bash Expected Output
Basic: openai/gpt-4o-mini
Performance: anthropic/claude-3-5-sonnet-20241022
Cost-focused: openai/gpt-4o-mini
```

</ResponseExample>

## Key Benefits

<Check>**No Inference Costs** - Get routing decisions without running actual completions</Check>

<Check>**Fast Decisions** - Sub-second response times for model selection</Check>

<Check>**Cost Optimization** - Balance performance and cost with the cost_bias parameter</Check>

<Check>**Function Calling Aware** - Automatically prioritizes compatible models when tools are provided</Check>

<Check>**Hybrid Support** - Mix cloud and custom models seamlessly</Check>

## Next Steps

<Columns cols={3}>
  <Card title="Chat Completions" icon="message-circle" href="/api-reference/chat-completions">
    Use the selected model for actual completions
  </Card>
  <Card title="OpenAI SDK Integration" icon="code" href="/integrations/openai-sdk">
    Integrate with your existing OpenAI SDK code
  </Card>
  <Card title="Intelligent Routing" icon="route" href="/features/intelligent-routing">
    Learn about Adaptive's routing algorithms
  </Card>
</Columns>