---
title: "Select Model"
api: "POST https://api.llmadaptive.uk/v1/select-model"
description: "Get intelligent model selection for any provider or infrastructure"
icon: "route"
---

Get Adaptive's intelligent model selection without using our inference. **Provider-agnostic design** - works with any models, any providers, any infrastructure.

## Why Use This?

**Use Adaptive's intelligence, run inference wherever you want:**

- **"I have my own OpenAI/Anthropic accounts"** - Get optimal model selection, pay your providers directly
- **"I run models on-premise"** - Get routing decisions for your local infrastructure
- **"I have enterprise contracts"** - Use your existing provider relationships with intelligent routing
- **"I need data privacy"** - Keep inference local while getting smart model selection

## Request

**Provider-agnostic format** - send your available models and prompt, get intelligent selection back.

<ParamField body="models" type="array" required>
  Array of available model specifications in `provider:model_name` format. **Adaptive automatically queries the [Model Registry](/api-reference/models) to fill in pricing, capabilities, and other details for known models.**

  <Expandable title="Model Specification Format">
    ```json
    ["openai/gpt-5-mini", "anthropic/claude-sonnet-4-5", "gemini/gemini-2.5-flash-lite"]
    ```
  </Expandable>
</ParamField>

<ParamField body="prompt" type="string" required>
  The prompt text to analyze for optimal model selection
</ParamField>



<ParamField body="cost_bias" type="number">
  Cost optimization preference (0.0 = cheapest, 1.0 = best performance) Default:
  Uses server configuration. Override to prioritize cost savings or performance
  for this specific selection.
</ParamField>



<ParamField body="model_router_cache" type="object">
  Semantic cache configuration for this request
  <Expandable title="Semantic Cache Configuration">
    <ParamField body="enabled" type="boolean">
      Override whether to use semantic caching for this specific request
      (overrides server configuration)
    </ParamField>

    <ParamField body="semantic_threshold" type="number">
      Override similarity threshold for cache hits (0.0-1.0, higher = more
      strict matching)
    </ParamField>
  </Expandable>
</ParamField>

## Response

<ResponseField name="selected_model" type="object">
  **Selected model details** Complete model information for the chosen model
  <Expandable title="RegistryModel Object">
    <ResponseField name="id" type="integer">
      Unique model identifier
    </ResponseField>

    <ResponseField name="author" type="string">
      Model provider/author (e.g., "openai", "anthropic", "gemini")
    </ResponseField>

    <ResponseField name="model_name" type="string">
      Model identifier (e.g., "gpt-5-mini", "claude-sonnet-4-5")
    </ResponseField>

    <ResponseField name="display_name" type="string">
      Human-readable model name
    </ResponseField>

    <ResponseField name="description" type="string">
      Model description and capabilities
    </ResponseField>

    <ResponseField name="context_length" type="integer">
      Maximum context window in tokens
    </ResponseField>

    <ResponseField name="pricing" type="object">
      Cost information for the model
    </ResponseField>

    <ResponseField name="providers" type="array">
      Available endpoints for this model
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="alternatives" type="array">
  **Alternative models** (optional) Fallback model options if the primary selection is unavailable. Each alternative is a complete RegistryModel object.
</ResponseField>

<ResponseField name="cache_tier" type="string">
  **Cache hit information** Indicates if the selection came from cache ("semantic_exact", "semantic_similar", or empty if not cached)
</ResponseField>

## Quick Examples

### "Known models - just specify what you have"

```bash
curl https://api.llmadaptive.uk/v1/select-model \
  -H "X-Stainless-API-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "models": [
      "openai/gpt-5-mini",
      "anthropic/claude-sonnet-4-5",
      "gemini/gemini-2.5-flash-lite"
    ],
    "prompt": "Hello, how are you?"
  }'

# Response:
{
  "selected_model": {
    "id": 123,
    "author": "openai",
    "model_name": "gpt-5-mini",
    "display_name": "GPT-5 Mini",
    "context_length": 128000
  }
}
```

### "Just specify providers - let Adaptive choose"

```bash
curl https://api.llmadaptive.uk/v1/select-model \
  -H "X-Stainless-API-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "models": [
      "openai/gpt-5-mini",
      "anthropic/claude-sonnet-4-5"
    ],
    "prompt": "Write a complex analysis of market trends"
  }'

# Response:
{
  "selected_model": {
    "id": 456,
    "author": "anthropic",
    "model_name": "claude-sonnet-4-5",
    "display_name": "Claude Sonnet 4.5",
    "context_length": 200000
  },
  "alternatives": [
    {
      "id": 123,
      "author": "openai",
      "model_name": "gpt-5-mini",
      "display_name": "GPT-5 Mini",
      "context_length": 128000
    }
  ]
}
```

### "Test cost optimization"

```javascript
// Will cost_bias actually pick cheaper models?
const response = await fetch("/api/v1/select-model", {
  method: "POST",
  headers: { "X-Stainless-API-Key": apiKey },
  body: JSON.stringify({
    models: ["openai/gpt-4.1-nano", "openai/gpt-5-mini"],
    prompt: "Analyze this complex dataset and provide insights...",
    cost_bias: 0.1, // Maximize cost savings
  }),
});

if (!response.ok) {
  const errorBody = await response.text();
  throw new Error(`HTTP ${response.status}: ${errorBody}`);
}

const result = await response.json();
console.log(result);
// Check if it picked the cheaper model despite complexity
```



### "Compare different configurations"

```python
import requests
import os

# Configuration
BASE_URL = "https://api.yourdomain.com"  # Replace with your actual domain
API_TOKEN = os.getenv("ADAPTIVE_API_TOKEN", "your-api-token-here")  # Set via environment variable
TIMEOUT = 30  # Request timeout in seconds

# Define available models
models = [
    "openai/gpt-4.1-nano",
    "openai/gpt-5-mini"
]

base_request = {
    "models": models,
    "prompt": "Write Python code to analyze customer data"
}

# Headers for authentication
headers = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

# Test cost-focused vs performance-focused
configs = [
    {"cost_bias": 0.1, "name": "cost-optimized"},
    {"cost_bias": 0.9, "name": "performance-focused"}
]

for config in configs:
    try:
        response = requests.post(
            f"{BASE_URL}/api/v1/select-model",
            json={
                **base_request,
                "cost_bias": config["cost_bias"]
            },
            headers=headers,
            timeout=TIMEOUT
        )

        # Check if request was successful
        if response.ok:
            result = response.json()
            print(f"{config['name']}: {result['selected_model']['author']}/{result['selected_model']['model_name']}")
        else:
            print(f"Error for {config['name']}: HTTP {response.status_code} - {response.text}")

    except requests.exceptions.Timeout:
        print(f"Timeout error for {config['name']}: Request took longer than {TIMEOUT} seconds")
    except requests.exceptions.ConnectionError:
        print(f"Connection error for {config['name']}: Unable to connect to {BASE_URL}")
    except requests.exceptions.RequestException as e:
        print(f"Request error for {config['name']}: {e}")
    except Exception as e:
        print(f"Unexpected error for {config['name']}: {e}")
```

## Real-World Integration Patterns

### 1. Use Your Own Provider Accounts

```javascript
// Define your available models with your own pricing
const availableModels = [
  "openai/gpt-5-mini",
  "anthropic/claude-sonnet-4-5",
  "gemini/gemini-2.5-flash-lite",
];

// Get intelligent selection
const selection = await fetch("/api/v1/select-model", {
  method: "POST",
  headers: { "X-Stainless-API-Key": adaptiveKey },
  body: JSON.stringify({
    models: availableModels,
    prompt: userMessage,
  }),
});

const result = await selection.json();

// Route to your own provider accounts
if (result.selected_model.author === "openai") {
  const completion = await yourOpenAI.chat.completions.create({
    model: result.selected_model.model_name,
    messages: [{ role: "user", content: userMessage }],
  });
} else if (result.selected_model.author === "anthropic") {
  const completion = await yourAnthropic.messages.create({
    model: result.selected_model.model_name,
    messages: [{ role: "user", content: userMessage }],
    max_tokens: 4096,
  });
}
```

### 2. Multi-Provider Routing

```javascript
// Tell Adaptive about your preferred providers (plus a fallback)
const res = await fetch("https://api.llmadaptive.uk/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": adaptiveKey,
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      "anthropic/claude-opus-4-1",
      "z-ai/glm-4.6",
      "openai/gpt-5-mini", // Cloud fallback
    ],
    prompt: userMessage,
  }),
});
const selection = await res.json();

// Route to the right provider using provider/model
if (selection.provider === "anthropic") {
  await yourAnthropic.messages.create({
    model: selection.model,
    messages: [{ role: "user", content: userMessage }],
    max_tokens: 4096,
  });
} else if (selection.provider === "openai") {
  await yourOpenAI.chat.completions.create({
    model: selection.model,
    messages: [{ role: "user", content: userMessage }],
  });
} else if (selection.provider === "z-ai") {
  await yourZAIClient.generate({
    model: selection.model,
    messages: [{ role: "user", content: userMessage }],
  });
}
```

### 3. Enterprise Contract Optimization

```javascript
// Maximize usage of your enterprise contracts
const res = await fetch("https://api.llmadaptive.uk/v1/select-model", {
  method: "POST",
  headers: {
    "X-Stainless-API-Key": adaptiveKey,
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    models: [
      "anthropic/claude-opus-4-1", // Your enterprise contract
      "openai/gpt-5-mini", // Your enterprise contract
      "gemini/gemini-2.5-flash-lite", // Pay-per-use fallback
    ],
    prompt: userMessage,
    cost_bias: 0.8,
  }),
});
const selection = await res.json();

// Always use your own accounts
const client = yourProviderClients[selection.provider];
const completion = await client.create({
  model: selection.model,
  messages: [{ role: "user", content: userMessage }],
});
```

### 4. Data Privacy & Compliance

```javascript
// Prefer privacy-focused providers while keeping prompts redacted
const selection = await selectModel({
  models: [
    "z-ai/glm-4.6",              // Zero-retention provider
    "anthropic/claude-haiku-4-5" // Privacy-focused Anthropic model
  ],
  prompt: "NON_SENSITIVE_TASK_DESCRIPTION",
  // Don't send actual sensitive data to Adaptive
});

// Route sensitive content only to providers you approve
if (selection.provider === "z-ai") {
  const result = await yourZAIClient.generate({
    model: selection.model,
    messages: actualSensitiveData,
  });
} else if (selection.provider === "anthropic") {
  const result = await yourAnthropic.messages.create({
    model: selection.model,
    messages: actualSensitiveData,
  });
}
```

## Understanding the Response

### What You Get Back

```json
{
  "selected_model": {
    "id": 123,
    "author": "openai",
    "model_name": "gpt-5-mini",
    "display_name": "GPT-5 Mini",
    "context_length": 128000,
    "pricing": {...},
    "providers": [...]
  },
  "alternatives": [
    {
      "id": 456,
      "author": "anthropic",
      "model_name": "claude-sonnet-4-5",
      "display_name": "Claude Sonnet 4.5",
      "context_length": 200000
    }
  ],
  "cache_tier": "semantic_exact"
}
```

### Key Insights

- **`selected_model`** - Complete model information for the chosen model including pricing, capabilities, and available endpoints
- **`alternatives`** - Fallback model options with full model details
- **`cache_tier`** - Indicates if the selection came from semantic cache ("semantic_exact", "semantic_similar", or empty)

## Common Patterns

### Before/After Comparison

```javascript
// See what changes with different parameters
const baseline = await selectModel(request);
const withConstraints = await selectModel({
  ...request,
  cost_bias: 0.1,
});

console.log(`Baseline: ${baseline.selected_model.model_name}`);
console.log(`Cost-optimized: ${withConstraints.selected_model.model_name}`);
```

### Validate Your Setup

```javascript
// Make sure your routing rules work
const shouldUseCheap = await fetch(
  "https://api.llmadaptive.uk/v1/select-model",
  {
    method: "POST",
    headers: {
      "X-Stainless-API-Key": adaptiveKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      models: ["openai/gpt-4.1-nano", "openai/gpt-5-mini"],
      prompt: "Hi",
    }),
  },
).then((r) => r.json());

const shouldUseExpensive = await fetch(
  "https://api.llmadaptive.uk/v1/select-model",
  {
    method: "POST",
    headers: {
      "X-Stainless-API-Key": adaptiveKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      models: ["openai/gpt-4.1-nano", "openai/gpt-5-mini"],
      prompt: "Analyze this complex dataset...",
    }),
  },
).then((r) => r.json());

// Verify different complexity tasks get different models
```

## Authentication

Same as chat completions:

```bash
# Any of these work
-H "X-Stainless-API-Key: your-key"
-H "Authorization: Bearer your-key"
```

## No Inference = Fast & Cheap

This endpoint:

- ✅ **Fast** - No LLM inference, just routing logic
- ✅ **Cheap** - Doesn't count against token usage
- ✅ **Accurate** - Uses exact same selection logic as real completions

Perfect for testing, debugging, and cost planning without burning through your budget.
