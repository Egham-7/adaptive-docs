---
title: "Select Model"
api: "POST https://api.llmadaptive.uk/v1/select-model"
description: "Get intelligent model selection for any prompt"
---

Get optimal model recommendations for your prompts without running inference.

## Quick Start

<CodeGroup>
```bash cURL
curl https://api.llmadaptive.uk/v1/select-model \
  -H "Authorization: Bearer apk_123456" \
  -d '{
    "models": ["openai/gpt-4", "anthropic/claude-3"],
    "prompt": "Write a Python function to sort a list"
  }'
```

```python Python
import requests

response = requests.post(
    "https://api.llmadaptive.uk/v1/select-model",
    headers={"Authorization": "Bearer apk_123456"},
    json={
        "models": ["openai/gpt-4", "anthropic/claude-3"],
        "prompt": "Write a Python function to sort a list"
    }
)
```
</CodeGroup>

## Parameters

<ParamField body="models" type="array" required>
  Array of available models: `["openai/gpt-4", "anthropic/claude-3"]`
</ParamField>

<ParamField body="prompt" type="string" required>
  The prompt text to analyze
</ParamField>

<ParamField body="cost_bias" type="number">
  Cost vs performance balance (0.0-1.0). Default: 0.5
</ParamField>

## Response

```json
{
  "selected_model": "openai/gpt-4",
  "confidence": 0.85,
  "reasoning": "Best balance of cost and capability for coding tasks",
  "estimated_cost": {
    "prompt": 0.00003,
    "completion": 0.00006
  }
}
```
      Override whether to use semantic caching for this specific request
      (overrides server configuration)
    </ParamField>

    <ParamField body="semantic_threshold" type="number">
      Override similarity threshold for cache hits (0.0-1.0, higher = more
      strict matching)
    </ParamField>
  </Expandable>
</ParamField>

## Response

<ResponseField name="selected_model" type="object">
  **Selected model details** Complete model information for the chosen model
  <Expandable title="RegistryModel Object">
    <ResponseField name="id" type="integer">
      Unique model identifier
    </ResponseField>

    <ResponseField name="author" type="string">
      Model provider/author (e.g., "openai", "anthropic", "gemini")
    </ResponseField>

    <ResponseField name="model_name" type="string">
      Model identifier (e.g., "gpt-5-mini", "claude-sonnet-4-5")
    </ResponseField>

    <ResponseField name="display_name" type="string">
      Human-readable model name
    </ResponseField>

    <ResponseField name="description" type="string">
      Model description and capabilities
    </ResponseField>

    <ResponseField name="context_length" type="integer">
      Maximum context window in tokens
    </ResponseField>

    <ResponseField name="pricing" type="object">
      Cost information for the model
    </ResponseField>

    <ResponseField name="providers" type="array">
      Available endpoints for this model
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="alternatives" type="array">
  **Alternative models** (optional) Fallback model options if the primary selection is unavailable. Each alternative is a complete RegistryModel object.
</ResponseField>



## Authentication

Same as chat completions:

```bash
-H "Authorization: Bearer apk_123456"
```

## No Inference = Fast & Cheap

This endpoint:

- ✅ **Fast** - No LLM inference, just routing logic
- ✅ **Cheap** - Doesn't count against token usage
- ✅ **Accurate** - Uses exact same selection logic as real completions

Perfect for testing, debugging, and cost planning without burning through your budget.
