---
title: "Select Model"
api: "POST https://api.llmadaptive.uk/v1/select-model"
description: "Get intelligent model selection for any prompt"
---

Get optimal model recommendations for your prompts without running inference.

## Quick Start

<CodeGroup>
```bash cURL
curl https://api.llmadaptive.uk/v1/select-model \
  -H "Authorization: Bearer apk_123456" \
  -d '{
    "models": ["openai/gpt-4", "anthropic/claude-3-5-sonnet-20241022"],
    "prompt": "Write a Python function to sort a list"
  }'
```

```python Python
import requests

response = requests.post(
    "https://api.llmadaptive.uk/v1/select-model",
    headers={"Authorization": "Bearer apk_123456"},
    json={
        "models": ["openai/gpt-4", "anthropic/claude-3-5-sonnet-20241022"],
        "prompt": "Write a Python function to sort a list"
    }
)
```
</CodeGroup>

## Parameters

<ParamField body="models" type="array">
  **Optional** - Array of available model specifications in `provider/model_name` format.

  If provided, these models will be used for selection. If omitted, Adaptive automatically uses all models from your configured providers via the [Model Registry](/api-reference/models).

  <Expandable title="Model Specification Format">
    ```json
    ["openai/gpt-4", "anthropic/claude-3-5-sonnet-20241022", "google/gemini-2.0-flash-exp"]
    ```
  </Expandable>
</ParamField>

<ParamField body="prompt" type="string" required>
  The prompt text to analyze
</ParamField>

<ParamField body="cost_bias" type="number">
  Cost optimization preference (0.0 = cheapest, 1.0 = best performance).

  Default: Uses your project's adaptive configuration. Override to prioritize cost savings or performance for this specific selection.
</ParamField>

<ParamField body="semantic_cache_threshold" type="number">
  Semantic cache similarity threshold (0.0-1.0, higher = stricter matching).

  Default: Uses your project's adaptive configuration. Override to control how similar prompts need to be for cache hits.
</ParamField>

## Response

<ResponseField name="selected_model" type="object">
  **Selected model details** Complete model information for the chosen model
  <Expandable title="RegistryModel Object">
    <ResponseField name="id" type="integer">
      Unique model identifier
    </ResponseField>

    <ResponseField name="author" type="string">
      Model provider/author (e.g., "openai", "anthropic", "gemini")
    </ResponseField>

    <ResponseField name="model_name" type="string">
      Model identifier (e.g., "gpt-4", "claude-3-5-sonnet-20241022")
    </ResponseField>

    <ResponseField name="display_name" type="string">
      Human-readable model name
    </ResponseField>

    <ResponseField name="description" type="string">
      Model description and capabilities
    </ResponseField>

    <ResponseField name="context_length" type="integer">
      Maximum context window in tokens
    </ResponseField>

    <ResponseField name="pricing" type="object">
      Cost information for the model
    </ResponseField>

    <ResponseField name="providers" type="array">
      Available endpoints for this model
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="alternatives" type="array">
  **Alternative models** (optional) Fallback model options if the primary selection is unavailable. Each alternative is a complete RegistryModel object.
</ResponseField>

<ResponseField name="cache_tier" type="string">
  **Cache hit information** Indicates if the selection came from cache ("semantic_exact", "semantic_similar", or empty if not cached)
</ResponseField>

## Authentication

Same as chat completions:

```bash
-H "Authorization: Bearer apk_123456"
```

## Example Requests

### With Explicit Models

Specify exactly which models to consider:

```json
{
  "models": [
    "openai/gpt-4",
    "anthropic/claude-3-5-sonnet-20241022"
  ],
  "prompt": "Write a Python function to calculate fibonacci numbers",
  "cost_bias": 0.3,
  "semantic_cache_threshold": 0.9
}
```

### Auto-Discovery from Registry

Let Adaptive use all configured provider models:

```json
{
  "prompt": "Write a Python function to calculate fibonacci numbers",
  "cost_bias": 0.3
}
```

Adaptive will automatically fetch all available models from your configured providers (OpenAI, Anthropic, Google, etc.).

### Using Project Defaults

Use your project's adaptive configuration settings:

```json
{
  "prompt": "Write a Python function to calculate fibonacci numbers"
}
```

Uses `cost_bias` and `semantic_cache_threshold` from your project's adaptive configuration in the dashboard.

## No Inference = Fast & Cheap

This endpoint:

- ✅ **Fast** - No LLM inference, just routing logic
- ✅ **Cheap** - Doesn't count against token usage
- ✅ **Accurate** - Uses exact same selection logic as real completions

Perfect for testing, debugging, and cost planning without burning through your budget.
