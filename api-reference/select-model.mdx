---
title: "Select Model"
api: "POST https://api.llmadaptive.uk/v1/select-model"
description: "Get intelligent model selection for any provider or infrastructure"
icon: "route"
---

Get Adaptive's intelligent model selection without using our inference. **Provider-agnostic design** - works with any models, any providers, any infrastructure.

## Why Use This?

**Use Adaptive's intelligence, run inference wherever you want:**

- **"I have my own OpenAI/Anthropic accounts"** - Get optimal model selection, pay your providers directly
- **"I run models on-premise"** - Get routing decisions for your local infrastructure
- **"I have enterprise contracts"** - Use your existing provider relationships with intelligent routing
- **"I need data privacy"** - Keep inference local while getting smart model selection

## Request

**Provider-agnostic format** - send your available models and prompt, get intelligent selection back.

<ParamField body="models" type="array" required>
  Array of available model specifications in `provider:model_name` format. **Adaptive automatically queries the [Model Registry](/api-reference/models) to fill in pricing, capabilities, and other details for known models.**

  <Expandable title="Model Specification Format">
    ```json
    ["openai/gpt-5-mini", "anthropic/claude-sonnet-4-5", "gemini/gemini-2.5-flash-lite"]
    ```
  </Expandable>
</ParamField>

<ParamField body="prompt" type="string" required>
  The prompt text to analyze for optimal model selection
</ParamField>



<ParamField body="cost_bias" type="number">
  Cost optimization preference (0.0 = cheapest, 1.0 = best performance) Default:
  Uses server configuration. Override to prioritize cost savings or performance
  for this specific selection.
</ParamField>



<ParamField body="model_router_cache" type="object">
  Semantic cache configuration for this request
  <Expandable title="Semantic Cache Configuration">
    <ParamField body="enabled" type="boolean">
      Override whether to use semantic caching for this specific request
      (overrides server configuration)
    </ParamField>

    <ParamField body="semantic_threshold" type="number">
      Override similarity threshold for cache hits (0.0-1.0, higher = more
      strict matching)
    </ParamField>
  </Expandable>
</ParamField>

## Response

<ResponseField name="selected_model" type="object">
  **Selected model details** Complete model information for the chosen model
  <Expandable title="RegistryModel Object">
    <ResponseField name="id" type="integer">
      Unique model identifier
    </ResponseField>

    <ResponseField name="author" type="string">
      Model provider/author (e.g., "openai", "anthropic", "gemini")
    </ResponseField>

    <ResponseField name="model_name" type="string">
      Model identifier (e.g., "gpt-5-mini", "claude-sonnet-4-5")
    </ResponseField>

    <ResponseField name="display_name" type="string">
      Human-readable model name
    </ResponseField>

    <ResponseField name="description" type="string">
      Model description and capabilities
    </ResponseField>

    <ResponseField name="context_length" type="integer">
      Maximum context window in tokens
    </ResponseField>

    <ResponseField name="pricing" type="object">
      Cost information for the model
    </ResponseField>

    <ResponseField name="providers" type="array">
      Available endpoints for this model
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="alternatives" type="array">
  **Alternative models** (optional) Fallback model options if the primary selection is unavailable. Each alternative is a complete RegistryModel object.
</ResponseField>

<ResponseField name="cache_tier" type="string">
  **Cache hit information** Indicates if the selection came from cache ("semantic_exact", "semantic_similar", or empty if not cached)
</ResponseField>

## Authentication

Same as chat completions:

```bash
# Any of these work
-H "X-Stainless-API-Key: your-key"
-H "Authorization: Bearer your-key"
```

## No Inference = Fast & Cheap

This endpoint:

- ✅ **Fast** - No LLM inference, just routing logic
- ✅ **Cheap** - Doesn't count against token usage
- ✅ **Accurate** - Uses exact same selection logic as real completions

Perfect for testing, debugging, and cost planning without burning through your budget.
