---
title: 'Chat Completions'
api: 'POST https://api.llmadaptive.uk/v1/chat/completions'
description: 'OpenAI-compatible completions with 60-90% cost savings'
icon: "comments"
---

<Note>
üí° **Quick Start**: Same as OpenAI API, but use `model: ""` for intelligent routing and automatic cost savings
</Note>

## 30-Second Setup

**1. Authentication**: Use your Adaptive API key (either format works)
```
X-Stainless-API-Key: your-adaptive-api-key
# OR
Authorization: Bearer your-adaptive-api-key
```

**2. Model Selection**: Leave empty for smart routing
```json
{
  "model": "",  // ‚Üê This enables intelligent routing
  "messages": [...]
}
```

**That's it!** Your requests automatically save 60-90% while maintaining quality.

## Essential Parameters

<ParamField body="model" type="string" required>
  **For intelligent routing**: Use `""` (empty string) to automatically select the best model for cost and quality
  
  **For specific models**: Use provider:model format like `"anthropic:claude-3-sonnet"` or `"openai:gpt-4"`
</ParamField>

<ParamField body="messages" type="array" required>
  Array of message objects. Same format as OpenAI.
  
  <Expandable title="Message Format">
    ```json
    [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Hello!"}
    ]
    ```
    
    **Roles**: `system`, `user`, `assistant`, `tool`
  </Expandable>
</ParamField>

<ParamField body="temperature" type="number">
  Creativity level: `0` = focused, `1` = balanced, `2` = creative. Default: `1`
</ParamField>

<ParamField body="max_completion_tokens" type="integer">
  Maximum response length in tokens. Leave unset for automatic sizing.
</ParamField>

<ParamField body="stream" type="boolean">
  Enable streaming responses. Default: `false`
</ParamField>

## Smart Routing & Cost Control

<ParamField body="model_router" type="object">
  **Control intelligent routing** to optimize cost and performance
  
  <Expandable title="Quick Examples">
    ```javascript
    // Prefer cost savings (80% cheaper on average)
    model_router: { cost_bias: 0.1 }
    
    // Balanced cost and quality
    model_router: { cost_bias: 0.5 }
    
    // Prefer best performance
    model_router: { cost_bias: 0.9 }
    
    // Limit to specific providers
    model_router: {
      models: [
        { provider: "openai" },
        { provider: "anthropic" }
      ]
    }
    ```
  </Expandable>
  
  <Expandable title="Full Configuration">
    <ParamField body="cost_bias" type="number">
      Balance cost vs performance: `0` = cheapest, `1` = best quality. Default: `0.5`
    </ParamField>
    
    <ParamField body="models" type="array">
      Allowed providers/models. Examples:
      - `{ provider: "openai" }` - All OpenAI models
      - `{ provider: "anthropic", model_name: "claude-3-sonnet" }` - Specific model
    </ParamField>
    
    <ParamField body="complexity_threshold" type="number">
      Override automatic complexity detection (0-1)
    </ParamField>
    
    <ParamField body="token_threshold" type="integer">
      Override automatic token counting threshold
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="fallback" type="object">
  **Provider backup** when primary fails
  
  <Expandable title="Fallback Options">
    ```javascript
    // Try providers one by one (cheaper)
    fallback: { mode: "sequential" }
    
    // Try multiple providers at once (faster)
    fallback: { mode: "race" }
    ```
    
    <ParamField body="mode" type="string">
      `"sequential"` (cheaper) or `"race"` (faster)
    </ParamField>
  </Expandable>
</ParamField>



<Accordion title="üìã All Standard OpenAI Parameters">

### Core Parameters

<ParamField body="max_tokens" type="integer">
  **Deprecated** - Maximum number of tokens to generate. Use `max_completion_tokens` instead.
</ParamField>

<ParamField body="max_completion_tokens" type="integer">
  Maximum number of tokens that can be generated for completion, including reasoning tokens.
</ParamField>

<ParamField body="stream" type="boolean">
  Whether to stream the response. Default: `false`
</ParamField>

<ParamField body="top_p" type="number">
  Nucleus sampling parameter between 0 and 1. Default: `1`
</ParamField>

<ParamField body="frequency_penalty" type="number">
  Penalty for token frequency. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="presence_penalty" type="number">
  Penalty for token presence. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="n" type="integer">
  Number of chat completion choices to generate. Default: `1`
</ParamField>

<ParamField body="seed" type="integer">
  Seed for deterministic sampling. Helps ensure reproducible results.
</ParamField>

<ParamField body="stop" type="string | array">
  Up to 4 sequences where the API will stop generating tokens.
</ParamField>

<ParamField body="user" type="string">
  Unique identifier for end-user to help detect abuse and improve caching.
</ParamField>

### Advanced Parameters

<ParamField body="logprobs" type="boolean">
  Whether to return log probabilities of output tokens. Default: `false`
</ParamField>

<ParamField body="top_logprobs" type="integer">
  Number of most likely tokens to return at each position (0-20). Requires `logprobs: true`.
</ParamField>

<ParamField body="logit_bias" type="object">
  Modify likelihood of specified tokens. Maps token IDs to bias values (-100 to 100).
</ParamField>

<ParamField body="response_format" type="object">
  Format for model output. Supports JSON schema for structured outputs.
  
  <Expandable title="Response Format Options">
    <ParamField body="type" type="string">
      Either `json_object` or `json_schema`
    </ParamField>
    
    <ParamField body="json_schema" type="object">
      JSON schema definition when using `json_schema` type
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="service_tier" type="string">
  Latency tier for processing. Options: `auto`, `default`, `flex`
</ParamField>

<ParamField body="store" type="boolean">
  Whether to store output for model distillation or evals. Default: `false`
</ParamField>

<ParamField body="metadata" type="object">
  Set of 16 key-value pairs for storing additional information about the request.
</ParamField>

### Audio and Multimodal

<ParamField body="modalities" type="array">
  Output types to generate. Options: `["text"]`, `["audio"]`, or `["text", "audio"]`
</ParamField>

<ParamField body="audio" type="object">
  Parameters for audio output when `modalities` includes `"audio"`.
</ParamField>

### Reasoning Models (o-series)

<ParamField body="reasoning_effort" type="string">
  **o-series models only** - Effort level for reasoning: `low`, `medium`, or `high`
</ParamField>

### Function Calling

<ParamField body="tools" type="array">
  Array of tool definitions for function calling. Maximum 128 functions.
  
  <Expandable title="Tool Definition">
    <ParamField body="type" type="string">
      Tool type, currently only `function` supported
    </ParamField>
    
    <ParamField body="function" type="object">
      Function definition with name, description, and parameters schema
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="tool_choice" type="string | object">
  Controls tool usage: `none`, `auto`, `required`, or specific tool selection
</ParamField>

<ParamField body="parallel_tool_calls" type="boolean">
  Whether to enable parallel function calling. Default: `true`
</ParamField>

<ParamField body="function_call" type="string | object">
  **Deprecated** - Use `tool_choice` instead. Controls function calling behavior.
</ParamField>

### Web Search

<ParamField body="web_search_options" type="object">
  Options for web search tool functionality.
</ParamField>

### Streaming Options

<ParamField body="stream_options" type="object">
  Additional options for streaming responses.
  
  <Expandable title="Stream Options">
    <ParamField body="include_usage" type="boolean">
      Whether to include usage statistics in streaming response
    </ParamField>
  </Expandable>
</ParamField>

### Prediction and Caching

<ParamField body="prediction" type="object">
  Static predicted output content for regeneration scenarios.
</ParamField>

### Adaptive-Specific Parameters

<ParamField body="model_router" type="object">
  Configuration for intelligent routing and provider selection.
  
  <Expandable title="Model Router Config">
    <ParamField body="models" type="ModelCapability[]">
      Array of model capabilities to consider for routing. Can be simplified or detailed:
      
      **Simple formats:**
      - `{ "provider": "openai" }` - Use all OpenAI models
      - `{ "provider": "anthropic", "model_name": "claude-3-sonnet-20240229" }` - Use specific model
      
      **Custom models require all parameters:**
      
      <Expandable title="Model Capability Object">
        <ParamField body="provider" type="string" required>
          Provider name: `"openai"`, `"anthropic"`, `"google"`, `"groq"`, `"deepseek"`, `"mistral"`, `"grok"`, `"huggingface"`
        </ParamField>
        
        <ParamField body="model_name" type="string">
          Specific model identifier (e.g., `"gpt-4"`, `"claude-3-sonnet"`). **Required for custom models or specific model selection**
        </ParamField>
        
        <ParamField body="cost_per_1m_input_tokens" type="number">
          Cost per 1 million input tokens in USD. **Required for custom models**
        </ParamField>
        
        <ParamField body="cost_per_1m_output_tokens" type="number">
          Cost per 1 million output tokens in USD. **Required for custom models**
        </ParamField>
        
        <ParamField body="max_context_tokens" type="integer">
          Maximum context window size in tokens. **Required for custom models**
        </ParamField>
        
        <ParamField body="max_output_tokens" type="integer">
          Maximum output tokens the model can generate. **Required for custom models**
        </ParamField>
        
        <ParamField body="supports_tool_calling" type="boolean">
          Whether the model supports function/tool calling. **Required for custom models**
        </ParamField>
        
        <ParamField body="description" type="string">
          Human-readable description of the model
        </ParamField>
        
        <ParamField body="languages_supported" type="string[]">
          Array of supported language codes
        </ParamField>
        
        <ParamField body="model_size_params" type="string">
          Model size information (e.g., `"7B"`, `"70B"`)
        </ParamField>
        
        <ParamField body="latency_tier" type="string">
          Expected latency: `"low"`, `"medium"`, `"high"`
        </ParamField>
        
        <ParamField body="task_type" type="string">
          Optimal task type: `"Open QA"`, `"Closed QA"`, `"Summarization"`, `"Text Generation"`, `"Code Generation"`, `"Chatbot"`, `"Classification"`, `"Rewrite"`, `"Brainstorming"`, `"Extraction"`, `"Other"`
        </ParamField>
        
        <ParamField body="complexity" type="string">
          Model complexity tier: `"low"`, `"medium"`, `"high"`
        </ParamField>
      </Expandable>
    </ParamField>
    
    <ParamField body="cost_bias" type="number">
      Bias towards cost optimization. Range: 0.0-1.0 where 0.0 = cheapest, 1.0 = best performance
    </ParamField>
    
    <ParamField body="complexity_threshold" type="number">
      Threshold for task complexity routing decisions. Range: 0.0-1.0
    </ParamField>
    
    <ParamField body="token_threshold" type="integer">
      Token count threshold for model selection. Positive integer.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="fallback" type="object">
  Configuration for provider fallback behavior. Fallback is disabled by default (empty/omitted), enabled when mode is specified.
  
  <Expandable title="Fallback Config">
    <ParamField body="mode" type="string">
      Fallback strategy: `"sequential"` or `"race"`. Empty/omitted = disabled, specified = enabled.
    </ParamField>
    
    <ParamField body="timeout_ms" type="integer">
      Timeout in milliseconds for fallback operations
    </ParamField>
    
    <ParamField body="max_retries" type="integer">
      Maximum number of retry attempts
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="provider_configs" type="object">
  Configuration for custom providers. Required when using custom providers in your model list.
  
  <Expandable title="Provider Config Object">
    <ParamField body="base_url" type="string" required>
      API base URL for the custom provider (e.g., `"https://api.custom.com/v1"`)
    </ParamField>
    
    <ParamField body="api_key" type="string" required>
      Full API key for authentication with the custom provider
    </ParamField>
    
    <ParamField body="auth_type" type="string">
      Authentication type: `"bearer"`, `"api_key"`, `"basic"`, or `"custom"`. Default: `"bearer"`
    </ParamField>
    
    <ParamField body="auth_header_name" type="string">
      Custom authentication header name. Default: `"Authorization"`
    </ParamField>
    
    <ParamField body="headers" type="object">
      Additional headers to send with requests to the custom provider
    </ParamField>
    
    <ParamField body="timeout_ms" type="integer">
      Request timeout in milliseconds. Range: 1000-120000. Default: 30000
    </ParamField>
    
    <ParamField body="rate_limit_rpm" type="integer">
      Rate limit in requests per minute. Range: 1-100000
    </ParamField>
    
    <ParamField body="health_endpoint" type="string">
      Health check endpoint for monitoring provider availability
    </ParamField>
    
    <ParamField body="retry_config" type="object">
      Custom retry configuration for failed requests
    </ParamField>
  </Expandable>
</ParamField>

</Accordion>

## Advanced Configuration

<Accordion title="üîß Custom Providers & Enterprise Features">

### Custom Providers

</Accordion>

## Response

<ResponseField name="id" type="string">
  Unique identifier for the completion
</ResponseField>

<ResponseField name="object" type="string">
  Object type, always `chat.completion`
</ResponseField>

<ResponseField name="created" type="integer">
  Unix timestamp of creation
</ResponseField>

<ResponseField name="model" type="string">
  Model used for the completion
</ResponseField>

<ResponseField name="provider" type="string">
  **Adaptive addition:** Which provider was selected (e.g., "openai", "anthropic")
</ResponseField>

<ResponseField name="choices" type="array">
  Array of completion choices
  
  <Expandable title="Choice Object">
    <ResponseField name="index" type="integer">
      Index of the choice
    </ResponseField>
    
    <ResponseField name="message" type="object">
      The generated message
      
      <Expandable title="Message Object">
        <ResponseField name="role" type="string">
          Role of the message, always "assistant"
        </ResponseField>
        
        <ResponseField name="content" type="string">
          The content of the message
        </ResponseField>
        
        <ResponseField name="tool_calls" type="array">
          Tool calls made by the model (if any)
        </ResponseField>
      </Expandable>
    </ResponseField>
    
    <ResponseField name="finish_reason" type="string">
      Reason completion finished: `stop`, `length`, `tool_calls`, or `content_filter`
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="usage" type="object">
  Token usage statistics
  
  <Expandable title="Usage Object">
    <ResponseField name="prompt_tokens" type="integer">
      Number of tokens in the prompt
    </ResponseField>
    
    <ResponseField name="completion_tokens" type="integer">
      Number of tokens in the completion
    </ResponseField>
    
    <ResponseField name="total_tokens" type="integer">
      Total tokens used
    </ResponseField>
    
    <ResponseField name="cache_tier" type="string">
      **Adaptive addition:** Cache tier used for this response. Possible values:
      - `"semantic_exact"` - Exact semantic cache match
      - `"semantic_similar"` - Similar semantic cache match
      - Omitted when no cache is used
    </ResponseField>
  </Expandable>
</ResponseField>

## Live Examples

<Tip>
üí° **Try These Examples**: Copy-paste ready code that works immediately. Each example shows the cost savings in action.
</Tip>

### 1. Simple Chat ‚Üí 97% Cost Savings

<Info>
**Cost Comparison:** Simple question routes to Gemini Flash
**OpenAI Direct:** $3.00 per 1M input tokens
**Adaptive Smart:** Gemini Flash ($0.075/1M) + Overhead ($0.10/1M input, $0.20/1M output)
**Savings: 97%** (total ~$0.10/1M vs $3.00/1M)
</Info>

<CodeGroup>

```javascript JavaScript - Copy & Run
const completion = await openai.chat.completions.create({
  model: '',  // ‚Üê Smart routing enabled
  messages: [
    { role: 'user', content: 'Explain quantum computing simply' }
  ],
});

console.log(completion.choices[0].message.content);
console.log(`Provider used: ${completion.provider}`);  // See which was chosen
console.log(`Cache tier: ${completion.usage.cache_tier || 'none'}`);
```

```python Python - Copy & Run
completion = client.chat.completions.create(
    model="",  # ‚Üê Smart routing enabled
    messages=[
        {"role": "user", "content": "Explain quantum computing simply"}
    ]
)

print(completion.choices[0].message.content)
print(f"Provider used: {completion.provider}")  # See which was chosen
print(f"Cache tier: {completion.usage.cache_tier or 'none'}")
```

```bash cURL - Copy & Run
curl https://api.llmadaptive.uk/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-Stainless-API-Key: $ADAPTIVE_API_KEY" \
  -d '{
    "model": "",
    "messages": [
      {"role": "user", "content": "Explain quantum computing simply"}
    ]
  }'
# Response includes "provider" field showing which model was selected
```

</CodeGroup>

### 2. Complex Analysis ‚Üí 85% Cost Savings

<Info>
**Cost Comparison:** Complex task routes to DeepSeek Reasoner
**OpenAI Direct:** $15.00 per 1M input tokens
**Adaptive Smart:** DeepSeek ($1.00/1M) + Overhead ($0.10/1M input, $0.20/1M output)
**Savings: 85%** (total ~$1.30/1M vs $15.00/1M)
</Info>

<CodeGroup>

```javascript JavaScript - Advanced Prompt
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { 
      role: 'user', 
      content: 'Analyze the economic implications of quantum computing on cryptocurrency security, considering both short-term disruptions and long-term adaptations. Include specific recommendations for blockchain protocols.' 
    }
  ],
});

// Complex prompts automatically route to premium models when needed
console.log(`Routed to: ${completion.provider}`);  // Likely Claude or DeepSeek
```

```python Python - Research Task
completion = client.chat.completions.create(
    model="",
    messages=[
        {
            "role": "user", 
            "content": "Conduct a comprehensive analysis of recent developments in quantum error correction, focusing on surface codes and their practical implementation challenges in NISQ devices."
        }
    ]
)

# Research tasks get premium models automatically
print(f"Routed to: {completion.provider}")  # Best model for the task
```

</CodeGroup>

### With Intelligent Routing Configuration

```javascript
// Simple provider selection
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Write a Python function to sort a list' }
  ],
  model_router: {
    models: [
      { provider: "anthropic" }, // All Anthropic models
      { provider: "openai", model_name: "gpt-4" } // Specific OpenAI model
    ],
    cost_bias: 0.2, // Prefer cost savings
    complexity_threshold: 0.3,
    token_threshold: 1000
  },
  fallback: {
    mode: 'sequential'  // Enabled by specifying mode
  }
});
```

### Using Custom Providers

```javascript
// Custom provider example
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Explain machine learning concepts' }
  ],
  model_router: {
    models: [
      { provider: "openai" }, // Standard provider
      { 
        provider: "my-custom-llm", // Custom provider
        model_name: "custom-model-v1",
        cost_per_1m_input_tokens: 2.0,
        cost_per_1m_output_tokens: 6.0,
        max_context_tokens: 16000,
        max_output_tokens: 4000,
        supports_tool_calling: true,
        task_type: "Text Generation",
        complexity: "medium"
      }
    ],
    cost_bias: 0.5
  },
  
  // Configure each custom provider
  provider_configs: {
    "my-custom-llm": {
      base_url: "https://api.mycustom.com/v1",
      api_key: "sk-custom-api-key-here",
      auth_type: "bearer",
      headers: {
        "X-Custom-Header": "value"
      },
      timeout_ms: 45000
    }
  }
});
```

### Customizing Standard Providers

You can also customize standard providers (OpenAI, Anthropic, etc.) with custom base URLs, API keys, and settings:

```javascript
// Override standard provider configuration
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Hello from custom OpenAI endpoint!' }
  ],
  model_router: {
    models: [
      { provider: "openai" }, // Will use custom config below
      { provider: "anthropic" } // Will also use custom config
    ]
  },
  
  // Custom configurations for standard providers
  provider_configs: {
    "openai": {
      base_url: "https://my-custom-openai-proxy.com/v1",
      api_key: "sk-my-custom-openai-key",
      timeout_ms: 60000,
      headers: {
        "X-Proxy-Key": "proxy-auth-123"
      }
    },
    "anthropic": {
      base_url: "https://my-anthropic-proxy.com/v1",
      api_key: "sk-ant-custom-key",
      timeout_ms: 45000
    }
  }
});
```

### Streaming Response

```javascript
const stream = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Tell me a story about space exploration' }
  ],
  stream: true
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

### Function Calling

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'What\'s the weather like in San Francisco?' }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'get_weather',
        description: 'Get current weather for a location',
        parameters: {
          type: 'object',
          properties: {
            location: {
              type: 'string',
              description: 'City and state, e.g. San Francisco, CA'
            }
          },
          required: ['location']
        }
      }
    }
  ]
});
```

### Vision (Multimodal)

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'What\'s in this image?' },
        {
          type: 'image_url',
          image_url: {
            url: 'https://example.com/image.jpg'
          }
        }
      ]
    }
  ],
  modalities: ['text'] // Can also include 'audio' for supported models
});
```

### Advanced Configuration with All Parameters

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Explain machine learning concepts' }
  ],
  
  // Core parameters
  temperature: 0.7,
  max_completion_tokens: 1000,
  top_p: 0.9,
  frequency_penalty: 0.1,
  presence_penalty: 0.1,
  n: 1,
  seed: 12345,
  stop: ['\n\n'],
  user: 'user-123',
  
  // Advanced parameters
  logprobs: true,
  top_logprobs: 5,
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'explanation',
      schema: {
        type: 'object',
        properties: {
          concept: { type: 'string' },
          explanation: { type: 'string' }
        }
      }
    }
  },
  service_tier: 'auto',
  store: false,
  metadata: {
    session_id: 'abc123',
    user_type: 'premium'
  },
  
  // Reasoning models (o-series)
  reasoning_effort: 'medium',
  
  // Function calling
  tools: [
    {
      type: 'function',
      function: {
        name: 'search_knowledge',
        description: 'Search knowledge base for information',
        parameters: {
          type: 'object',
          properties: {
            query: { type: 'string' }
          }
        }
      }
    }
  ],
  tool_choice: 'auto',
  parallel_tool_calls: true,
  
  // Streaming
  stream: false,
  stream_options: {
    include_usage: true
  },
  
  // Adaptive-specific
  model_router: {
    models: [
      { provider: "openai" }, // Use all OpenAI models
      { provider: "anthropic", model_name: "claude-3-sonnet-20240229" }, // Specific model
      // Custom model example (all params required):
      {
        provider: "my-custom-provider",
        model_name: "custom-model-v1",
        cost_per_1m_input_tokens: 5.0,
        cost_per_1m_output_tokens: 10.0,
        max_context_tokens: 32000,
        max_output_tokens: 2048,
        supports_tool_calling: false,
        task_type: "Text Generation",
        complexity: "medium"
      }
    ],
    cost_bias: 0.3,
    complexity_threshold: 0.5,
    token_threshold: 2000
  },
  
  provider_configs: {
    "my-custom-provider": {
      base_url: "https://api.custom.com/v1",
      api_key: "sk-custom-key-123",
      auth_type: "bearer",
      headers: {
        "Custom-Header": "custom-value"
      },
      timeout_ms: 30000,
      rate_limit_rpm: 1000
    }
  },
  
  fallback: {
    mode: 'sequential'  // Enabled by specifying mode
  }
});
```

## Response Examples

### Cache Tier Tracking

The `usage.cache_tier` field shows which cache served your response:

```json
// Semantic cache hit
{
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 8, 
    "total_tokens": 18,
    "cache_tier": "semantic_exact"
  }
}

// No cache used
{
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 10,
    "total_tokens": 18
    // cache_tier omitted
  }
}
```

## Error Handling & Troubleshooting

<Warning>
üõ†Ô∏è **Quick Fix Guide**: Most issues have simple solutions. Here's how to resolve them fast.
</Warning>

### ‚ö° Instant Fixes

<AccordionGroup>
<Accordion title="üîë Authentication Error (401)">
**Problem**: `{"error": {"message": "Invalid API key", "type": "authentication_error"}}`

**Instant Solutions**:
1. **Check header format**:
   ```javascript
   // ‚úÖ Correct
   headers: { "X-Stainless-API-Key": "your-adaptive-key" }
   // OR
   headers: { "Authorization": "Bearer your-adaptive-key" }
   
   // ‚ùå Wrong
   headers: { "X-API-Key": "your-key" }  // Wrong header name
   ```

2. **Verify your key**: Copy-paste from [llmadaptive.uk](https://www.llmadaptive.uk) dashboard
3. **Check environment variables**:
   ```bash
   echo $ADAPTIVE_API_KEY  # Should show your key
   ```

**Working Example**:
```javascript
const openai = new OpenAI({
  apiKey: process.env.ADAPTIVE_API_KEY,  // ‚Üê Make sure this is set
  baseURL: 'https://api.llmadaptive.uk/v1'
});
```
</Accordion>

<Accordion title="üìã Invalid Request (400)">
**Problem**: `{"error": {"message": "Invalid request", "type": "invalid_request_error"}}`

**Common Causes & Fixes**:

1. **Empty messages array**:
   ```javascript
   // ‚ùå Wrong
   messages: []
   
   // ‚úÖ Correct  
   messages: [{ role: "user", content: "Hello!" }]
   ```

2. **Missing required fields**:
   ```javascript
   // ‚ùå Wrong
   { role: "user" }  // Missing content
   
   // ‚úÖ Correct
   { role: "user", content: "Your message here" }
   ```

3. **Invalid model_router config**:
   ```javascript
   // ‚ùå Wrong - custom model missing required fields
   model_router: {
     models: [{ provider: "custom-provider" }]  // Missing details
   }
   
   // ‚úÖ Correct - all required fields for custom models
   model_router: {
     models: [{
       provider: "custom-provider",
       model_name: "model-v1",
       cost_per_1m_input_tokens: 2.0,
       cost_per_1m_output_tokens: 6.0,
       max_context_tokens: 16000,
       max_output_tokens: 4000,
       supports_tool_calling: true,
       task_type: "Text Generation",
       complexity: "medium"
     }]
   }
   ```
</Accordion>

<Accordion title="‚è±Ô∏è Rate Limited (429)">
**Problem**: `{"error": {"message": "Rate limit exceeded", "type": "rate_limit_error"}}`

**Immediate Actions**:
1. **Wait and retry**: Rate limits reset every minute
2. **Implement exponential backoff**:
   ```javascript
   async function callWithRetry(requestFn, maxRetries = 3) {
     for (let i = 0; i < maxRetries; i++) {
       try {
         return await requestFn();
       } catch (error) {
         if (error.status === 429 && i < maxRetries - 1) {
           await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
         } else {
           throw error;
         }
       }
     }
   }
   ```

3. **Upgrade your plan** at [llmadaptive.uk](https://www.llmadaptive.uk) for higher limits
4. **Use caching** to reduce requests:
   ```javascript
   semantic_cache: { enabled: true }
   ```
</Accordion>

<Accordion title="üîß Custom Provider Issues">
**Problem**: Custom provider not working or failing

**Checklist**:
1. **Provider configuration must be complete**:
   ```javascript
   provider_configs: {
     "my-provider": {
       base_url: "https://api.example.com/v1",  // ‚úÖ Required
       api_key: "sk-your-key",                  // ‚úÖ Required
       auth_type: "bearer",                     // ‚úÖ Good practice
       timeout_ms: 30000                       // ‚úÖ Recommended
     }
   }
   ```

2. **Model definition must include all fields**:
   ```javascript
   models: [{
     provider: "my-provider",
     model_name: "model-name",                    // ‚úÖ Required
     cost_per_1m_input_tokens: 2.0,              // ‚úÖ Required
     cost_per_1m_output_tokens: 6.0,             // ‚úÖ Required
     max_context_tokens: 16000,                  // ‚úÖ Required
     max_output_tokens: 4000,                    // ‚úÖ Required
     supports_tool_calling: false,           // ‚úÖ Required
     task_type: "Text Generation",               // ‚úÖ Required
     complexity: "medium"                        // ‚úÖ Required
   }]
   ```

3. **Test the provider directly first**:
   ```bash
   curl https://api.your-provider.com/v1/chat/completions \
     -H "Authorization: Bearer your-key" \
     -d '{"model": "model-name", "messages": [...]}'
   ```
</Accordion>
</AccordionGroup>

### Error Response Format

<ResponseField name="error" type="object">
  Standard error object format
  
  <Expandable title="Error Structure">
    ```json
    {
      "error": {
        "message": "Human-readable error description",
        "type": "authentication_error", 
        "code": "invalid_api_key"
      }
    }
    ```
    
    <ResponseField name="message" type="string">
      Clear description of what went wrong
    </ResponseField>
    
    <ResponseField name="type" type="string">
      Error category: `invalid_request_error`, `authentication_error`, `permission_error`, `rate_limit_error`, `server_error`
    </ResponseField>
    
    <ResponseField name="code" type="string">
      Specific error code for programmatic handling
    </ResponseField>
  </Expandable>
</ResponseField>

### üö® Emergency Troubleshooting

<Warning>
**Service Down?**  
1. Check our status page: [status.llmadaptive.uk](https://status.llmadaptive.uk)  
2. Join our Discord: [discord.gg/adaptive](https://discord.gg/adaptive)  
3. Email support: info@llmadaptive.uk
</Warning>

## Rate Limits

| Plan | Requests per Minute | Tokens per Minute |
|------|-------------------|-------------------|
| Free | 100 | 10,000 |
| Pro | 1,000 | 100,000 |
| Enterprise | Custom | Custom |

Rate limits are applied per API key and reset every minute.

## Best Practices

<CardGroup cols={2}>
  <Card title="Model Selection" icon="brain">
    Use empty string `""` for model to enable intelligent routing and cost savings
  </Card>
  <Card title="Cost Control" icon="dollar-sign">
    Use `cost_bias` parameter to balance cost vs performance for your use case
  </Card>
  <Card title="Custom Providers" icon="plug">
    When using custom providers, always include their configuration in `provider_configs`
  </Card>
  <Card title="Error Handling" icon="shield-check">
    Always implement proper error handling for network and API failures
  </Card>
</CardGroup>