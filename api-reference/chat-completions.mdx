---
title: 'Chat Completions'
api: 'POST https://api.llmadaptive.uk/v1/chat/completions'
description: 'OpenAI-compatible completions with 60-90% cost savings'
icon: "/images/logos/openai.svg"
---

<Note>
üí° **Quick Start**: Same as OpenAI API, but use `model: "adaptive/auto"` for intelligent routing and automatic cost savings
</Note>

## 30-Second Setup

**1. Authentication**: Use your Adaptive API key (either format works)
```
Authorization: Bearer apk_123456
```

**2. Model Selection**: Leave empty for smart routing
```json
{
  "model": "adaptive/auto",  // ‚Üê This enables intelligent routing
  "messages": [...]
}
```

**That's it!** Your requests automatically save 60-90% while maintaining quality.

## SDK Examples

<CodeGroup>
```javascript Node.js
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.ADAPTIVE_API_KEY || 'your-adaptive-api-key',
  baseURL: 'https://api.llmadaptive.uk/v1'
});

const response = await client.chat.completions.create({
  model: 'adaptive/auto', // Empty for intelligent routing
  messages: [{ role: 'user', content: 'Hello!' }]
});

console.log(response.choices[0].message.content);
```

```python Python
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("ADAPTIVE_API_KEY", "your-adaptive-api-key"),
    base_url="https://api.llmadaptive.uk/v1"
)

response = client.chat.completions.create(
    model="adaptive/auto",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```bash cURL
curl https://api.llmadaptive.uk/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer apk_123456" \
  -d '{
    "model": "adaptive/auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
</CodeGroup>

## Essential Parameters

<ParamField body="model" type="string" required>
  **For intelligent routing**: Use `""` (empty string) to automatically select the best model for cost and quality
  
  **For specific models**: Use author/model format like `"anthropic/claude-sonnet-4-5"` or `"openai/gpt-5-mini"`
</ParamField>

<ParamField body="messages" type="array" required>
  Array of message objects. Same format as OpenAI.
  
  <Expandable title="Message Format">
    ```json
    [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Hello!"}
    ]
    ```
    
    **Roles**: `system`, `user`, `assistant`, `tool`
  </Expandable>
</ParamField>

<ParamField body="temperature" type="number">
  Creativity level: `0` = focused, `1` = balanced, `2` = creative. Default: `1`
</ParamField>

<ParamField body="max_completion_tokens" type="integer">
  Maximum response length in tokens. Leave unset for automatic sizing.
</ParamField>

<ParamField body="stream" type="boolean">
  Enable streaming responses. Default: `false`
</ParamField>

## Smart Routing & Cost Control

<ParamField body="model_router" type="object">
  **Control intelligent routing** to optimize cost and performance
  
  <Expandable title="Quick Examples">
    ```javascript
    // Prefer cost savings (80% cheaper on average)
    model_router: { cost_bias: 0.1 }
    
    // Balanced cost and quality
    model_router: { cost_bias: 0.5 }
    
    // Prefer best performance
    model_router: { cost_bias: 0.9 }
    
    // Limit to specific providers
    model_router: {
      models: [
        "openai/gpt-5-mini",
        "anthropic/claude-sonnet-4-5"
      ]
    }
    ```
  </Expandable>
  
  <Expandable title="Full Configuration">
    <ParamField body="cost_bias" type="number">
      Balance cost vs performance: `0` = cheapest, `1` = best quality. Default: `0.5`
    </ParamField>
    
    <ParamField body="models" type="array">
      Allowed providers/models. Examples:
      - `"openai/gpt-5-mini"` - Specific model
      - `"anthropic/claude-sonnet-4-5"` - Specific model
      - `"gemini/gemini-2.5-flash-lite"` - Specific model
    </ParamField>
    
    <ParamField body="complexity_threshold" type="number">
      Override automatic complexity detection (0-1)
    </ParamField>
    
    <ParamField body="token_threshold" type="integer">
      Override automatic token counting threshold
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="fallback" type="object">
  **Provider backup** when primary fails
  
  <Expandable title="Fallback Options">
    ```javascript
    // Try providers one by one (cheaper)
    fallback: { mode: "sequential" }
    
    // Try multiple providers at once (faster)
    fallback: { mode: "race" }
    ```
    
    <ParamField body="mode" type="string">
      `"sequential"` (cheaper) or `"race"` (faster)
    </ParamField>
  </Expandable>
</ParamField>



<Accordion title="üìã All Standard OpenAI Parameters">

### Core Parameters

<ParamField body="max_tokens" type="integer">
  **Deprecated** - Maximum number of tokens to generate. Use `max_completion_tokens` instead.
</ParamField>

<ParamField body="max_completion_tokens" type="integer">
  Maximum number of tokens that can be generated for completion, including reasoning tokens.
</ParamField>

<ParamField body="stream" type="boolean">
  Whether to stream the response. Default: `false`
</ParamField>

<ParamField body="top_p" type="number">
  Nucleus sampling parameter between 0 and 1. Default: `1`
</ParamField>

<ParamField body="frequency_penalty" type="number">
  Penalty for token frequency. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="presence_penalty" type="number">
  Penalty for token presence. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="n" type="integer">
  Number of chat completion choices to generate. Default: `1`
</ParamField>

<ParamField body="seed" type="integer">
  Seed for deterministic sampling. Helps ensure reproducible results.
</ParamField>

<ParamField body="stop" type="string | array">
  Up to 4 sequences where the API will stop generating tokens.
</ParamField>

<ParamField body="user" type="string">
  Unique identifier for end-user to help detect abuse and improve caching.
</ParamField>

### Advanced Parameters

<ParamField body="logprobs" type="boolean">
  Whether to return log probabilities of output tokens. Default: `false`
</ParamField>

<ParamField body="top_logprobs" type="integer">
  Number of most likely tokens to return at each position (0-20). Requires `logprobs: true`.
</ParamField>

<ParamField body="logit_bias" type="object">
  Modify likelihood of specified tokens. Maps token IDs to bias values (-100 to 100).
</ParamField>

<ParamField body="response_format" type="object">
  Format for model output. Supports JSON schema for structured outputs.
  
  <Expandable title="Response Format Options">
    <ParamField body="type" type="string">
      Either `json_object` or `json_schema`
    </ParamField>
    
    <ParamField body="json_schema" type="object">
      JSON schema definition when using `json_schema` type
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="service_tier" type="string">
  Latency tier for processing. Options: `auto`, `default`, `flex`
</ParamField>

<ParamField body="store" type="boolean">
  Whether to store output for model distillation or evals. Default: `false`
</ParamField>

<ParamField body="metadata" type="object">
  Set of 16 key-value pairs for storing additional information about the request.
</ParamField>

### Audio and Multimodal

<ParamField body="modalities" type="array">
  Output types to generate. Options: `["text"]`, `["audio"]`, or `["text", "audio"]`
</ParamField>

<ParamField body="audio" type="object">
  Parameters for audio output when `modalities` includes `"audio"`.
</ParamField>

### Reasoning Models (o-series)

<ParamField body="reasoning_effort" type="string">
  **o-series models only** - Effort level for reasoning: `low`, `medium`, or `high`
</ParamField>

### Function Calling

<ParamField body="tools" type="array">
  Array of tool definitions for function calling. Maximum 128 functions.
  
  <Expandable title="Tool Definition">
    <ParamField body="type" type="string">
      Tool type, currently only `function` supported
    </ParamField>
    
    <ParamField body="function" type="object">
      Function definition with name, description, and parameters schema
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="tool_choice" type="string | object">
  Controls tool usage: `none`, `auto`, `required`, or specific tool selection
</ParamField>

<ParamField body="parallel_tool_calls" type="boolean">
  Whether to enable parallel function calling. Default: `true`
</ParamField>

<ParamField body="function_call" type="string | object">
  **Deprecated** - Use `tool_choice` instead. Controls function calling behavior.
</ParamField>

### Web Search

<ParamField body="web_search_options" type="object">
  Options for web search tool functionality.
</ParamField>

### Streaming Options

<ParamField body="stream_options" type="object">
  Additional options for streaming responses.
  
  <Expandable title="Stream Options">
    <ParamField body="include_usage" type="boolean">
      Whether to include usage statistics in streaming response
    </ParamField>
  </Expandable>
</ParamField>

### Prediction and Caching

<ParamField body="prediction" type="object">
  Static predicted output content for regeneration scenarios.
</ParamField>

### Adaptive-Specific Parameters

<ParamField body="model_router" type="object">
  Configuration for intelligent routing and provider selection.
  
  <Expandable title="Model Router Config">
    <ParamField body="models" type="ModelCapability[]">
      Array of model capabilities to consider for routing. Can be simplified or detailed:
      
      **Simple formats:**
      - `"openai/gpt-5-mini"` - Use specific OpenAI model
      - `"anthropic/claude-sonnet-4-5"` - Use specific Anthropic model
      
      **Custom models require all parameters:**
      
      <Expandable title="Model Capability Object">
        <ParamField body="provider" type="string" required>
          Provider name: `"openai"`, `"anthropic"`, `"gemini"`, `"z-ai"`
        </ParamField>
        
        <ParamField body="model_name" type="string">
          Specific model identifier (e.g., `"gpt-5-mini"`, `"claude-sonnet-4-5"`). **Required for specific model selection**
        </ParamField>
        
        <ParamField body="cost_per_1m_input_tokens" type="number">
          Cost per 1 million input tokens in USD.
        </ParamField>
        
        <ParamField body="cost_per_1m_output_tokens" type="number">
          Cost per 1 million output tokens in USD.
        </ParamField>

        <ParamField body="context_length" type="integer">
          Maximum context window size in tokens. Replaces deprecated `max_context_tokens`.
        </ParamField>

        <ParamField body="max_completion_tokens" type="integer">
          Maximum output tokens the model can generate. Replaces deprecated `max_output_tokens`.
        </ParamField>

        <ParamField body="supported_parameters" type="array">
          List of supported API parameters (e.g., ["temperature", "top_p", "tools"]). Replaces deprecated `supports_tool_calling` boolean.
        </ParamField>
        
        <ParamField body="description" type="string">
          Human-readable description of the model
        </ParamField>
        
        <ParamField body="languages_supported" type="string[]">
          Array of supported language codes
        </ParamField>
        
        <ParamField body="model_size_params" type="string">
          Model size information (e.g., `"7B"`, `"70B"`)
        </ParamField>
        
        <ParamField body="latency_tier" type="string">
          Expected latency: `"low"`, `"medium"`, `"high"`
        </ParamField>
        
        <ParamField body="task_type" type="string">
          Optimal task type: `"Open QA"`, `"Closed QA"`, `"Summarization"`, `"Text Generation"`, `"Code Generation"`, `"Chatbot"`, `"Classification"`, `"Rewrite"`, `"Brainstorming"`, `"Extraction"`, `"Other"`
        </ParamField>
        
        <ParamField body="complexity" type="string">
          Model complexity tier: `"low"`, `"medium"`, `"high"`
        </ParamField>
      </Expandable>
    </ParamField>
    
    <ParamField body="cost_bias" type="number">
      Bias towards cost optimization. Range: 0.0-1.0 where 0.0 = cheapest, 1.0 = best performance
    </ParamField>
    
    <ParamField body="complexity_threshold" type="number">
      Threshold for task complexity routing decisions. Range: 0.0-1.0
    </ParamField>
    
    <ParamField body="token_threshold" type="integer">
      Token count threshold for model selection. Positive integer.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="fallback" type="object">
  Configuration for provider fallback behavior. Fallback is disabled by default (empty/omitted), enabled when mode is specified.
  
  <Expandable title="Fallback Config">
    <ParamField body="mode" type="string">
      Fallback strategy: `"sequential"` or `"race"`. Empty/omitted = disabled, specified = enabled.
    </ParamField>
    
    <ParamField body="timeout_ms" type="integer">
      Timeout in milliseconds for fallback operations
    </ParamField>
    
    <ParamField body="max_retries" type="integer">
      Maximum number of retry attempts
    </ParamField>
  </Expandable>
 </ParamField>

</Accordion>

## Response

<ResponseField name="id" type="string">
  Unique identifier for the completion
</ResponseField>

<ResponseField name="object" type="string">
  Object type, always `chat.completion`
</ResponseField>

<ResponseField name="created" type="integer">
  Unix timestamp of creation
</ResponseField>

<ResponseField name="model" type="string">
  Model used for the completion
</ResponseField>



<ResponseField name="choices" type="array">
  Array of completion choices
  
  <Expandable title="Choice Object">
    <ResponseField name="index" type="integer">
      Index of the choice
    </ResponseField>
    
    <ResponseField name="message" type="object">
      The generated message
      
      <Expandable title="Message Object">
        <ResponseField name="role" type="string">
          Role of the message, always "assistant"
        </ResponseField>
        
        <ResponseField name="content" type="string">
          The content of the message
        </ResponseField>
        
        <ResponseField name="tool_calls" type="array">
          Tool calls made by the model (if any)
        </ResponseField>
      </Expandable>
    </ResponseField>
    
    <ResponseField name="finish_reason" type="string">
      Reason completion finished: `stop`, `length`, `tool_calls`, or `content_filter`
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="usage" type="object">
  Token usage statistics
  
  <Expandable title="Usage Object">
    <ResponseField name="prompt_tokens" type="integer">
      Number of tokens in the prompt
    </ResponseField>
    
    <ResponseField name="completion_tokens" type="integer">
      Number of tokens in the completion
    </ResponseField>
    
    <ResponseField name="total_tokens" type="integer">
      Total tokens used
    </ResponseField>
    

  </Expandable>
</ResponseField>

<CardGroup cols={2}>
  <Card title="Model Selection" icon="brain">
    Use empty string `""` for model to enable intelligent routing and cost savings
  </Card>
  <Card title="Cost Control" icon="dollar-sign">
    Use `cost_bias` parameter to balance cost vs performance for your use case
  </Card>
  <Card title="Custom Providers" icon="plug">
    When using custom providers, always include their configuration in `provider_configs`
  </Card>
  <Card title="Error Handling" icon="shield-check">
    Always implement proper error handling for network and API failures
  </Card>
</CardGroup>
