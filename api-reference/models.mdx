---
title: 'Models API'
description: 'Query the model registry to discover available LLM models and their capabilities'
api: 'GET /v1/models'
---

## Overview

The Models API provides access to Adaptive's comprehensive model registry, which contains detailed information about available LLM models including pricing, capabilities, context limits, and provider details.

**Use this API to:**
- Discover available models across all providers
- Get detailed pricing and capability information
- Filter models by provider
- Retrieve specific model details for integration

## Registry Model System

Adaptive maintains a centralized **Model Registry** that tracks comprehensive information about LLM models from multiple providers (OpenAI, Anthropic, Google, DeepSeek, Groq, and more).

### What is a Registry Model?

A Registry Model is a comprehensive data structure containing:

- **Identity**: Provider, model name, OpenRouter ID
- **Pricing**: Input/output token costs, per-request costs
- **Capabilities**: Context length, supported parameters, tool calling support
- **Architecture**: Modality, tokenizer, instruction format
- **Provider Info**: Top provider configuration, available endpoints
- **Metadata**: Display name, description, timestamps

### How the Registry Works

1. **Centralized Data Source**: The registry service maintains up-to-date model information
2. **Automatic Lookups**: When you specify a provider or model, Adaptive queries the registry
3. **Auto-Fill**: Known models automatically get pricing and capability data filled in

## Endpoints

### List All Models

<ParamField query="provider" type="string">
  Optional provider filter (e.g., "openai", "anthropic", "google")
</ParamField>

<RequestExample>
```bash cURL
curl https://api.llmadaptive.uk/v1/models \
  -H "Authorization: Bearer $ADAPTIVE_API_KEY"
```

```bash Filter by Provider
curl https://api.llmadaptive.uk/v1/models?provider=openai \
  -H "Authorization: Bearer $ADAPTIVE_API_KEY"
```

```python Python
import requests

headers = {
    "Authorization": f"Bearer {api_key}"
}

# List all models
response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers=headers
)
models = response.json()

# Filter by provider
response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=anthropic",
    headers=headers
)
anthropic_models = response.json()
```

```javascript JavaScript
const headers = {
  'Authorization': `Bearer ${apiKey}`
};

// List all models
const response = await fetch('https://api.llmadaptive.uk/v1/models', {
  headers
});
const models = await response.json();

// Filter by provider
const anthropicResponse = await fetch(
  'https://api.llmadaptive.uk/v1/models?provider=anthropic',
  { headers }
);
const anthropicModels = await anthropicResponse.json();
```
</RequestExample>

<ResponseExample>
```json 200 Success
[
  {
    "id": 1,
    "openrouter_id": "openai/gpt-5-mini",
    "provider": "openai",
    "model_name": "gpt-5-mini",
    "display_name": "GPT-5 Mini",
    "description": "Affordable and intelligent small model for fast, lightweight tasks",
    "context_length": 128000,
    "pricing": {
      "prompt": "0.00015",
      "completion": "0.0006",
      "request": "0",
      "image": "0"
    },
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"],
      "tokenizer": "o200k_base",
      "instruct_type": null
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "supported_parameters": [
      "temperature",
      "top_p",
      "max_tokens",
      "tools",
      "response_format",
      "seed"
    ],
    "default_parameters": {
      "temperature": 1.0
    },
    "endpoints": [
      {
        "name": "openai/gpt-5-mini",
        "model_name": "GPT-5 Mini",
        "context_length": 128000,
        "pricing": {
          "prompt": "0.00015",
          "completion": "0.0006",
          "request": "0",
          "image": "0.2890"
        },
        "provider_name": "OpenAI",
        "tag": "openai",
        "max_completion_tokens": 16384,
        "supported_parameters": [
          "temperature",
          "top_p",
          "tools",
          "response_format"
        ],
        "status": 0,
        "supports_implicit_caching": false
      }
    ],
    "created_at": "2025-01-15T10:30:00Z",
    "last_updated": "2025-01-20T14:45:00Z"
  }
]
```

```json 502 Bad Gateway
{
  "error": "Failed to fetch models from registry: connection timeout"
}
```
</ResponseExample>

### Get Model by Name

<ParamField path="id" type="string" required>
  Model identifier (e.g., "gpt-5-mini", "claude-sonnet-4-5")
</ParamField>

<RequestExample>
```bash cURL
curl https://api.llmadaptive.uk/v1/models/gpt-5-mini \
  -H "Authorization: Bearer $ADAPTIVE_API_KEY"
```

```python Python
import requests

headers = {
    "Authorization": f"Bearer {api_key}"
}

response = requests.get(
    "https://api.llmadaptive.uk/v1/models/gpt-5-mini",
    headers=headers
)
model = response.json()

print(f"Model: {model['display_name']}")
print(f"Context Length: {model['context_length']}")
print(f"Input Cost: ${model['pricing']['prompt']} per token")
```

```javascript JavaScript
const headers = {
  'Authorization': `Bearer ${apiKey}`
};

const response = await fetch(
  'https://api.llmadaptive.uk/v1/models/gpt-5-mini',
  { headers }
);
const model = await response.json();

console.log(`Model: ${model.display_name}`);
console.log(`Context Length: ${model.context_length}`);
console.log(`Input Cost: $${model.pricing.prompt} per token`);
```
</RequestExample>

<ResponseExample>
```json 200 Success
{
  "id": 1,
  "openrouter_id": "openai/gpt-5-mini",
  "provider": "openai",
  "model_name": "gpt-5-mini",
  "display_name": "GPT-5 Mini",
  "description": "Affordable and intelligent small model for fast, lightweight tasks",
  "context_length": 128000,
  "pricing": {
    "prompt": "0.00015",
    "completion": "0.0006",
    "request": "0",
    "image": "0"
  },
  "architecture": {
    "modality": "text+image->text",
    "input_modalities": ["text", "image"],
    "output_modalities": ["text"],
    "tokenizer": "o200k_base",
    "instruct_type": null
  },
  "top_provider": {
    "context_length": 128000,
    "max_completion_tokens": 16384,
    "is_moderated": true
  },
  "supported_parameters": [
    "temperature",
    "top_p",
    "max_tokens",
    "tools",
    "response_format",
    "seed"
  ],
  "default_parameters": {
    "temperature": 1.0
  },
  "endpoints": [
    {
      "name": "openai/gpt-5-mini",
      "model_name": "GPT-5 Mini",
      "context_length": 128000,
      "pricing": {
        "prompt": "0.00015",
        "completion": "0.0006",
        "request": "0",
        "image": "0.2890"
      },
      "provider_name": "OpenAI",
      "tag": "openai",
      "max_completion_tokens": 16384,
      "supported_parameters": [
        "temperature",
        "top_p",
        "tools",
        "response_format"
      ],
      "status": 0,
      "supports_implicit_caching": false
    }
  ],
  "created_at": "2025-01-15T10:30:00Z",
  "last_updated": "2025-01-20T14:45:00Z"
}
```

```json 404 Not Found
{
  "error": "Model 'unknown-model' not found"
}
```

```json 400 Bad Request
{
  "error": "Model name is required"
}
```
</ResponseExample>

## Response Schema

### RegistryModel Object

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Database ID (internal use) |
| `openrouter_id` | string | OpenRouter model identifier (primary lookup key) |
| `provider` | string | Provider name (e.g., "openai", "anthropic") |
| `model_name` | string | Model identifier for API calls |
| `display_name` | string | Human-readable model name |
| `description` | string | Model description and use cases |
| `context_length` | integer | Maximum context window size in tokens |
| `pricing` | object | Pricing information (see below) |
| `architecture` | object | Model architecture details (see below) |
| `top_provider` | object | Top provider configuration (see below) |
| `supported_parameters` | array | Supported API parameters |
| `default_parameters` | object | Default parameter values |
| `endpoints` | array | Available provider endpoints (see below) |
| `created_at` | string | Creation timestamp (ISO 8601) |
| `last_updated` | string | Last update timestamp (ISO 8601) |

### Pricing Object

| Field | Type | Description |
|-------|------|-------------|
| `prompt` | string | Cost per input token (USD, string format) |
| `completion` | string | Cost per output token (USD, string format) |
| `request` | string | Cost per request (optional) |
| `image` | string | Cost per image (optional) |
| `web_search` | string | Cost for web search (optional) |

**Note**: Pricing is in string format to preserve precision. Multiply by 1M for cost per million tokens.

### Architecture Object

| Field | Type | Description |
|-------|------|-------------|
| `modality` | string | Input/output modality (e.g., "text->text", "text+image->text") |
| `input_modalities` | array | Supported input types |
| `output_modalities` | array | Supported output types |
| `tokenizer` | string | Tokenizer used (e.g., "cl100k_base", "o200k_base") |
| `instruct_type` | string | Instruction format (e.g., "chatml", null) |

### TopProvider Object

| Field | Type | Description |
|-------|------|-------------|
| `context_length` | integer | Provider's context limit |
| `max_completion_tokens` | integer | Maximum output tokens |
| `is_moderated` | boolean | Whether content is moderated |

### Endpoint Object

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Full endpoint name |
| `model_name` | string | Display model name |
| `context_length` | integer | Context length for this endpoint |
| `pricing` | object | Endpoint-specific pricing |
| `provider_name` | string | Provider name |
| `tag` | string | Provider tag/slug |
| `max_completion_tokens` | integer | Max completion tokens |
| `supported_parameters` | array | Supported parameters |
| `status` | integer | Status code (0 = active) |
| `supports_implicit_caching` | boolean | Implicit caching support |

## Common Use Cases

### 1. Discover Available Models

Query all models to see what's available:

```python
import requests

response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers={"Authorization": f"Bearer {api_key}"}
)

models = response.json()
for model in models:
    print(f"{model['provider']}/{model['model_name']}: {model['display_name']}")
```

### 2. Compare Pricing Across Providers

Find the cheapest model for your use case:

```python
import requests

response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers={"Authorization": f"Bearer {api_key}"}
)

models = response.json()

# Calculate average cost per token
for model in models:
    input_cost = float(model['pricing']['prompt'])
    output_cost = float(model['pricing']['completion'])
    avg_cost = (input_cost + output_cost) / 2

    print(f"{model['display_name']}: ${avg_cost * 1_000_000:.2f} per 1M tokens")
```

### 3. Check Tool Calling Support

Find models that support function calling:

```python
import requests

response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers={"Authorization": f"Bearer {api_key}"}
)

models = response.json()

tool_calling_models = [
    model for model in models
    if "tools" in model.get('supported_parameters', []) or
       "functions" in model.get('supported_parameters', [])
]

print(f"Found {len(tool_calling_models)} models with tool calling support")
```

### 4. Get Provider-Specific Models

List all models from a specific provider:

```python
import requests

response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=anthropic",
    headers={"Authorization": f"Bearer {api_key}"}
)

anthropic_models = response.json()
for model in anthropic_models:
    print(f"{model['model_name']}: {model['context_length']} token context")
```

### 5. Validate Model Before Using

Check if a model exists and get its capabilities:

```python
import requests

model_name = "gpt-5-mini"

response = requests.get(
    f"https://api.llmadaptive.uk/v1/models/{model_name}",
    headers={"Authorization": f"Bearer {api_key}"}
)

if response.status_code == 200:
    model = response.json()
    print(f"✅ Model exists: {model['display_name']}")
    print(f"Context: {model['context_length']} tokens")
    print(f"Supports tools: {'tools' in model['supported_parameters']}")
else:
    print(f"❌ Model not found: {model_name}")
```

## Integration with Other APIs

### Use with Chat Completions

Combine with the [Chat Completions API](/api-reference/chat-completions) for intelligent routing:

```python
import requests

# 1. Query registry for available models
models_response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=openai",
    headers={"Authorization": f"Bearer {api_key}"}
)
available_models = models_response.json()

# 2. Use models in chat completion with intelligent routing
chat_response = requests.post(
    "https://api.llmadaptive.uk/v1/chat/completions",
    headers={"Authorization": f"Bearer {api_key}"},
    json={
        "model": "",  # Empty for intelligent routing
        "messages": [{"role": "user", "content": "Hello"}],
        "model_router": {
            "models": [
                f"{m['provider']}:{m['model_name']}"
                for m in available_models[:3]  # Use top 3 models
            ]
        }
    }
)
```

### Use with Select Model API

Combine with the [Select Model API](/api-reference/select-model) for explicit selection:

```python
import requests

# 1. Get models from registry
models_response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers={"Authorization": f"Bearer {api_key}"}
)
models = models_response.json()

# 2. Use select-model to choose best model for prompt
selection_response = requests.post(
    "https://api.llmadaptive.uk/v1/select-model",
    headers={"Authorization": f"Bearer {api_key}"},
    json={
        "prompt": "Write a Python function to process CSV files",
        "models": [
            f"{m['provider']}:{m['model_name']}"
            for m in models
            if "tools" in m.get("supported_parameters", [])
        ]
    }
)
```

## Best Practices

### 1. Cache Registry Data

Cache model information to reduce API calls:

```python
import requests
from datetime import datetime, timedelta

class ModelRegistry:
    def __init__(self, api_key):
        self.api_key = api_key
        self.cache = {}
        self.cache_expiry = None
        self.cache_duration = timedelta(hours=1)

    def get_models(self, provider=None):
        # Check cache
        if self.cache and self.cache_expiry and datetime.now() < self.cache_expiry:
            if provider:
                return [m for m in self.cache.get('models', [])
                       if m['provider'] == provider]
            return self.cache.get('models', [])

        # Fetch from API
        url = "https://api.llmadaptive.uk/v1/models"
        if provider:
            url += f"?provider={provider}"

        response = requests.get(
            url,
            headers={"Authorization": f"Bearer {self.api_key}"}
        )

        models = response.json()
        self.cache = {'models': models}
        self.cache_expiry = datetime.now() + self.cache_duration

        return models
```

### 2. Handle Registry Failures Gracefully

Always have fallback options:

```python
import requests

def get_models_with_fallback(api_key, provider=None):
    try:
        url = "https://api.llmadaptive.uk/v1/models"
        if provider:
            url += f"?provider={provider}"

        response = requests.get(
            url,
            headers={"Authorization": f"Bearer {api_key}"},
            timeout=5
        )
        response.raise_for_status()
        return response.json()

    except Exception as e:
        print(f"Registry query failed: {e}")
        # Return fallback models
        return [
            {"provider": "openai", "model_name": "gpt-5-mini"},
            {"provider": "anthropic", "model_name": "claude-sonnet-4-5"}
        ]
```

### 3. Filter by Capabilities

Select models based on your requirements:

```python
def find_models_by_criteria(api_key, min_context=100000, supports_tools=True):
    response = requests.get(
        "https://api.llmadaptive.uk/v1/models",
        headers={"Authorization": f"Bearer {api_key}"}
    )

    models = response.json()

    filtered = []
    for model in models:
        # Check context length
        if model['context_length'] < min_context:
            continue

        # Check tool support
        if supports_tools and 'tools' not in model.get('supported_parameters', []):
            continue

        filtered.append(model)

    return filtered
```

## Error Handling

| Status Code | Description | Solution |
|-------------|-------------|----------|
| 200 | Success | Process returned models |
| 400 | Bad Request | Check model ID parameter |
| 404 | Not Found | Model doesn't exist in registry |
| 502 | Bad Gateway | Registry service unavailable, use fallback |

## Related Documentation

- [Model Specification Reference](/api-reference/model-specification) - Complete field documentation
- [Chat Completions API](/api-reference/chat-completions) - Use models in chat completions
- [Select Model API](/api-reference/select-model) - Intelligent model selection
- [Intelligent Routing](/features/intelligent-routing) - How routing uses registry data
