---
title: 'Models API'
description: 'Query the model registry to discover available LLM models and their capabilities'
icon: "microchip"
api: 'GET /v1/models'
---

## Overview

The Models API provides access to Adaptive's comprehensive model registry, which contains detailed information about available LLM models including pricing, capabilities, context limits, and provider details.

**Use this API to:**
- Discover available models across all providers
- Get detailed pricing and capability information
- Filter models by provider
- Retrieve specific model details for integration

## Registry Model System

Adaptive maintains a centralized **Model Registry** that tracks comprehensive information about LLM models from multiple providers (OpenAI, Anthropic, Google, DeepSeek, Groq, and more).

### What is a Registry Model?

A Registry Model is a comprehensive data structure containing:

- **Identity**: Provider, model name, OpenRouter ID
- **Pricing**: Input/output token costs, per-request costs
- **Capabilities**: Context length, supported parameters, tool calling support
- **Architecture**: Modality, tokenizer, instruction format
- **Provider Info**: Top provider configuration, available endpoints
- **Metadata**: Display name, description, timestamps

### How the Registry Works

1. **Centralized Data Source**: The registry service maintains up-to-date model information
2. **Automatic Lookups**: When you specify a provider or model, Adaptive queries the registry
3. **Auto-Fill**: Known models automatically get pricing and capability data filled in

## Endpoints

### List All Models

<ParamField query="provider" type="string">
  Optional provider filter (e.g., "openai", "anthropic", "google")
</ParamField>

### Advanced Filtering

The Models API supports comprehensive filtering with repeatable query parameters:

<ParamField query="author" type="string[]">
  Filter by model author (repeatable). Example: `?author=openai&author=anthropic`
</ParamField>

<ParamField query="model_name" type="string[]">
  Filter by model name (repeatable). Example: `?model_name=gpt-4&model_name=claude-3`
</ParamField>

<ParamField query="input_modality" type="string[]">
  Filter by input modality (repeatable). Example: `?input_modality=text&input_modality=image`
</ParamField>

<ParamField query="output_modality" type="string[]">
  Filter by output modality (repeatable). Example: `?output_modality=text`
</ParamField>

<ParamField query="min_context_length" type="integer">
  Filter by minimum context length. Example: `?min_context_length=128000`
</ParamField>

<ParamField query="max_prompt_cost" type="string">
  Filter by maximum prompt cost. Example: `?max_prompt_cost=0.00001`
</ParamField>

<ParamField query="supported_param" type="string[]">
  Filter by required parameters (repeatable). Example: `?supported_param=tools&supported_param=vision`
</ParamField>

<ParamField query="status" type="integer">
  Filter by endpoint status (0=active). Example: `?status=0`
</ParamField>

<ParamField query="quantization" type="string[]">
  Filter by model quantization (repeatable). Example: `?quantization=fp16`
</ParamField>

<RequestExample>
```bash Advanced Filter Examples
# Find vision-capable OpenAI models with large context
curl "https://api.llmadaptive.uk/v1/models?author=openai&supported_param=vision&min_context_length=100000" \
  -H "Authorization: Bearer apk_123456"

# Find cost-effective models with multimodal input
curl "https://api.llmadaptive.uk/v1/models?input_modality=text&input_modality=image&max_prompt_cost=0.00001" \
  -H "Authorization: Bearer apk_123456"

# Find active models from multiple providers with tool support
curl "https://api.llmadaptive.uk/v1/models?provider=openai&provider=anthropic&supported_param=tools&status=0" \
  -H "Authorization: Bearer apk_123456"
```

```python Python - Advanced Filtering
import requests

headers = {"Authorization": f"Bearer {api_key}"}

# Find models matching multiple criteria
response = requests.get(
    "https://api.llmadaptive.uk/v1/models",
    headers=headers,
    params={
        "author": ["openai", "anthropic"],
        "input_modality": ["text", "image"],
        "supported_param": ["tools"],
        "min_context_length": 100000,
        "status": 0
    }
)

filtered_models = response.json()
print(f"Found {len(filtered_models)} models matching criteria")

for model in filtered_models:
    print(f"- {model['display_name']}: {model['context_length']} tokens, ${model['pricing']['prompt']}/token")
```
</RequestExample>

<Note>
  **Query Parameter Syntax**: For multiple values, repeat the parameter name:
  - ✅ Correct: `?author=openai&author=anthropic`
  - ❌ Incorrect: `?author=openai,anthropic` (comma-separated not supported)
</Note>

<RequestExample>
```bash List All Models
curl https://api.llmadaptive.uk/v1/models \
  -H "Authorization: Bearer apk_123456"
```

```bash Filter by Single Provider
curl "https://api.llmadaptive.uk/v1/models?provider=openai" \
  -H "Authorization: Bearer apk_123456"
```

```bash Filter by Multiple Providers (Repeat Parameter)
curl "https://api.llmadaptive.uk/v1/models?provider=openai&provider=anthropic" \
  -H "Authorization: Bearer apk_123456"
```

```python Python
import requests

headers = {"Authorization": f"Bearer {api_key}"}

# List all models
response = requests.get("https://api.llmadaptive.uk/v1/models", headers=headers)
models = response.json()

# Filter by single provider
response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=openai",
    headers=headers
)
openai_models = response.json()

# Filter by multiple providers (repeat parameter)
response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=openai&provider=anthropic",
    headers=headers
)
multi_provider_models = response.json()
```

```javascript JavaScript
const headers = {'Authorization': `Bearer ${apiKey}`};

// List all models
const response = await fetch('https://api.llmadaptive.uk/v1/models', { headers });
const models = await response.json();

// Filter by single provider
const openaiResponse = await fetch(
  'https://api.llmadaptive.uk/v1/models?provider=openai',
  { headers }
);
const openaiModels = await openaiResponse.json();

// Filter by multiple providers (repeat parameter)
const multiResponse = await fetch(
  'https://api.llmadaptive.uk/v1/models?provider=openai&provider=anthropic',
  { headers }
);
const multiProviderModels = await multiResponse.json();
```
</RequestExample>

<ResponseExample>
```json 200 Success
[
  {
    "id": 1,
    "openrouter_id": "openai/gpt-5-mini",
    "provider": "openai",
    "model_name": "gpt-5-mini",
    "display_name": "GPT-5 Mini",
    "description": "Affordable and intelligent small model for fast, lightweight tasks",
    "context_length": 128000,
    "pricing": {
      "prompt": "0.00015",
      "completion": "0.0006",
      "request": "0",
      "image": "0"
    },
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"],
      "tokenizer": "o200k_base",
      "instruct_type": null
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "supported_parameters": [
      "temperature",
      "top_p",
      "max_tokens",
      "tools",
      "response_format",
      "seed"
    ],
    "default_parameters": {
      "temperature": 1.0
    },
    "endpoints": [
      {
        "name": "openai/gpt-5-mini",
        "model_name": "GPT-5 Mini",
        "context_length": 128000,
        "pricing": {
          "prompt": "0.00015",
          "completion": "0.0006",
          "request": "0",
          "image": "0.2890"
        },
        "provider_name": "OpenAI",
        "tag": "openai",
        "max_completion_tokens": 16384,
        "supported_parameters": [
          "temperature",
          "top_p",
          "tools",
          "response_format"
        ],
        "status": 0,
        "supports_implicit_caching": false
      }
    ],
    "created_at": "2025-01-15T10:30:00Z",
    "last_updated": "2025-01-20T14:45:00Z"
  }
]
```

```json 502 Bad Gateway
{
  "error": "Failed to fetch models from registry: connection timeout"
}
```
</ResponseExample>

### Get Model by Name

<ParamField path="id" type="string" required>
  Model identifier (e.g., "gpt-5-mini", "claude-sonnet-4-5")
</ParamField>

<RequestExample>
```bash cURL
curl https://api.llmadaptive.uk/v1/models/gpt-5-mini \
  -H "Authorization: Bearer apk_123456"
```

```python Python
import requests

headers = {
    "Authorization": f"Bearer {api_key}"
}

response = requests.get(
    "https://api.llmadaptive.uk/v1/models/gpt-5-mini",
    headers=headers
)
model = response.json()

print(f"Model: {model['display_name']}")
print(f"Context Length: {model['context_length']}")
print(f"Input Cost: ${model['pricing']['prompt']} per token")
```

```javascript JavaScript
const headers = {
  'Authorization': `Bearer ${apiKey}`
};

const response = await fetch(
  'https://api.llmadaptive.uk/v1/models/gpt-5-mini',
  { headers }
);
const model = await response.json();

console.log(`Model: ${model.display_name}`);
console.log(`Context Length: ${model.context_length}`);
console.log(`Input Cost: $${model.pricing.prompt} per token`);
```
</RequestExample>

<ResponseExample>
```json 200 Success
{
  "id": 1,
  "openrouter_id": "openai/gpt-5-mini",
  "provider": "openai",
  "model_name": "gpt-5-mini",
  "display_name": "GPT-5 Mini",
  "description": "Affordable and intelligent small model for fast, lightweight tasks",
  "context_length": 128000,
  "pricing": {
    "prompt": "0.00015",
    "completion": "0.0006",
    "request": "0",
    "image": "0"
  },
  "architecture": {
    "modality": "text+image->text",
    "input_modalities": ["text", "image"],
    "output_modalities": ["text"],
    "tokenizer": "o200k_base",
    "instruct_type": null
  },
  "top_provider": {
    "context_length": 128000,
    "max_completion_tokens": 16384,
    "is_moderated": true
  },
  "supported_parameters": [
    "temperature",
    "top_p",
    "max_tokens",
    "tools",
    "response_format",
    "seed"
  ],
  "default_parameters": {
    "temperature": 1.0
  },
  "endpoints": [
    {
      "name": "openai/gpt-5-mini",
      "model_name": "GPT-5 Mini",
      "context_length": 128000,
      "pricing": {
        "prompt": "0.00015",
        "completion": "0.0006",
        "request": "0",
        "image": "0.2890"
      },
      "provider_name": "OpenAI",
      "tag": "openai",
      "max_completion_tokens": 16384,
      "supported_parameters": [
        "temperature",
        "top_p",
        "tools",
        "response_format"
      ],
      "status": 0,
      "supports_implicit_caching": false
    }
  ],
  "created_at": "2025-01-15T10:30:00Z",
  "last_updated": "2025-01-20T14:45:00Z"
}
```

```json 404 Not Found
{
  "error": "Model 'unknown-model' not found"
}
```

```json 400 Bad Request
{
  "error": "Model name is required"
}
```
</ResponseExample>

## Response Schema

### RegistryModel Object

| Field | Type | Description |
|-------|------|-------------|
| `id` | integer | Database ID (internal use) |
| `openrouter_id` | string | OpenRouter model identifier (primary lookup key) |
| `provider` | string | Provider name (e.g., "openai", "anthropic") |
| `model_name` | string | Model identifier for API calls |
| `display_name` | string | Human-readable model name |
| `description` | string | Model description and use cases |
| `context_length` | integer | Maximum context window size in tokens |
| `pricing` | object | Pricing information (see below) |
| `architecture` | object | Model architecture details (see below) |
| `top_provider` | object | Top provider configuration (see below) |
| `supported_parameters` | array | Supported API parameters |
| `default_parameters` | object | Default parameter values |
| `endpoints` | array | Available provider endpoints (see below) |
| `created_at` | string | Creation timestamp (ISO 8601) |
| `last_updated` | string | Last update timestamp (ISO 8601) |

### Pricing Object

| Field | Type | Description |
|-------|------|-------------|
| `prompt` | string | Cost per input token (USD, string format) |
| `completion` | string | Cost per output token (USD, string format) |
| `request` | string | Cost per request (optional) |
| `image` | string | Cost per image (optional) |
| `web_search` | string | Cost for web search (optional) |

**Note**: Pricing is in string format to preserve precision. Multiply by 1M for cost per million tokens.

### Architecture Object

| Field | Type | Description |
|-------|------|-------------|
| `modality` | string | Input/output modality (e.g., "text->text", "text+image->text") |
| `input_modalities` | array | Supported input types |
| `output_modalities` | array | Supported output types |
| `tokenizer` | string | Tokenizer used (e.g., "cl100k_base", "o200k_base") |
| `instruct_type` | string | Instruction format (e.g., "chatml", null) |

### TopProvider Object

| Field | Type | Description |
|-------|------|-------------|
| `context_length` | integer | Provider's context limit |
| `max_completion_tokens` | integer | Maximum output tokens |
| `is_moderated` | boolean | Whether content is moderated |

### Endpoint Object

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Full endpoint name |
| `model_name` | string | Display model name |
| `context_length` | integer | Context length for this endpoint |
| `pricing` | object | Endpoint-specific pricing |
| `provider_name` | string | Provider name |
| `tag` | string | Provider tag/slug |
| `max_completion_tokens` | integer | Max completion tokens |
| `supported_parameters` | array | Supported parameters |
| `status` | integer | Status code (0 = active) |
| `supports_implicit_caching` | boolean | Implicit caching support |

## Integration with Other APIs

### Use with Chat Completions

Combine with the [Chat Completions API](/api-reference/chat-completions) for intelligent routing:

```python
import requests

# 1. Query registry for available models
models_response = requests.get(
    "https://api.llmadaptive.uk/v1/models?provider=openai&provider=anthropic",
    headers={"Authorization": f"Bearer {api_key}"}
)
available_models = models_response.json()

# 2. Use models in chat completion with intelligent routing
chat_response = requests.post(
    "https://api.llmadaptive.uk/v1/chat/completions",
    headers={"Authorization": f"Bearer {api_key}"},
    json={
        "model": "adaptive/auto",  # Empty for intelligent routing
        "messages": [{"role": "user", "content": "Hello"}],
        "model_router": {
            "models": [
                f"{m['provider']}:{m['model_name']}"
                for m in available_models[:3]  # Use top 3 models
            ]
        }
    }
)
```

### Use with Select Model API

Combine with the [Select Model API](/api-reference/select-model) for explicit selection:

```python
import requests

# 1. Get models from registry with filtering
models_response = requests.get(
    "https://api.llmadaptive.uk/v1/models?supported_param=tools&min_context_length=100000",
    headers={"Authorization": f"Bearer {api_key}"}
)
models = models_response.json()

# 2. Use select-model to choose best model for prompt
selection_response = requests.post(
    "https://api.llmadaptive.uk/v1/select-model",
    headers={"Authorization": f"Bearer {api_key}"},
    json={
        "prompt": "Write a Python function to process CSV files",
        "models": [
            f"{m['provider']}/{m['model_name']}"
            for m in models
            if "tools" in m.get("supported_parameters", [])
        ]
    }
)
```

