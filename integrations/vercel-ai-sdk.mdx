---
title: "Vercel AI SDK Integration"
description: "Use Adaptive with the Vercel AI SDK for streamlined AI applications"
sidebarTitle: "Vercel AI SDK"
icon: "triangle"
---

## Overview

The Vercel AI SDK works seamlessly with Adaptive through two methods:

- **Adaptive Provider (Recommended):** Use the native `@adaptive-llm/adaptive-ai-provider` provider for built-in support.
- **OpenAI Provider:** Use Adaptive via `@ai-sdk/openai` with a custom base URL.

---

## Method 1: Adaptive Provider

### Installation

<CodeGroup>
```bash npm
npm install ai @adaptive-llm/adaptive-ai-provider
```

```bash yarn
yarn add ai @adaptive-llm/adaptive-ai-provider
```

```bash pnpm
pnpm add ai @adaptive-llm/adaptive-ai-provider
```
</CodeGroup>

### Basic Setup

<CodeGroup>
```javascript Basic Configuration
import { adaptive } from "@adaptive-llm/adaptive-ai-provider";

// Use default configuration
const model = adaptive();
```

```javascript Custom Configuration
import { createAdaptive } from "@adaptive-llm/adaptive-ai-provider";

const customAdaptive = createAdaptive({
  baseURL: "https://www.llmadaptive.uk/api/v1",
  apiKey: process.env.ADAPTIVE_API_KEY,
  headers: {
    "User-Agent": "MyApp/1.0",
  },
});
```
</CodeGroup>

---

## Method 2: OpenAI Provider

### Installation

<CodeGroup>
```bash npm
npm install ai @ai-sdk/openai
```

```bash yarn
yarn add ai @ai-sdk/openai
```

```bash pnpm
pnpm add ai @ai-sdk/openai
```
</CodeGroup>

### Configuration

<CodeGroup>
```javascript JavaScript/Node.js
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const adaptiveOpenAI = openai({
  apiKey: process.env.ADAPTIVE_API_KEY,
  baseURL: 'https://www.llmadaptive.uk/api/v1',
});

const { text } = await generateText({
  model: adaptiveOpenAI(''), // Empty string enables intelligent routing
  prompt: 'Explain quantum computing simply',
});
```

```typescript TypeScript
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const adaptiveOpenAI = openai({
  apiKey: process.env.ADAPTIVE_API_KEY!,
  baseURL: 'https://www.llmadaptive.uk/api/v1',
});

const { text } = await generateText({
  model: adaptiveOpenAI(''), // Empty string enables intelligent routing
  prompt: 'Explain quantum computing simply',
});

console.log(text);
```
</CodeGroup>

---

## Text Generation

<CodeGroup>
```javascript Adaptive Provider
import { generateText } from 'ai';
import { adaptive } from '@adaptive-llm/adaptive-ai-provider';

const { text } = await generateText({
  model: adaptive(),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

```javascript OpenAI Provider
import { generateText } from 'ai';

const { text } = await generateText({
  model: adaptiveOpenAI(''),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```
</CodeGroup>

---

## Streaming

<CodeGroup>
```javascript Adaptive Provider
import { streamText } from 'ai';
import { adaptive } from '@adaptive-llm/adaptive-ai-provider';

const { textStream } = await streamText({
  model: adaptive(),
  prompt: 'Explain machine learning step by step',
});

for await (const delta of textStream) {
  process.stdout.write(delta);
}
```

```javascript OpenAI Provider
import { streamText } from 'ai';

const { textStream } = await streamText({
  model: adaptiveOpenAI(''),
  prompt: 'Explain machine learning step by step',
});

for await (const delta of textStream) {
  process.stdout.write(delta);
}
```
</CodeGroup>

---

## React Chat Component

<CodeGroup>
```javascript useChat Hook
import { useChat } from 'ai/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat', // Your API route using Adaptive
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          <strong>{m.role}: </strong>
          {m.content}
        </div>
      ))}
      
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

```javascript API Route - Adaptive Provider
// pages/api/chat.js or app/api/chat/route.js
import { adaptive } from '@adaptive-llm/adaptive-ai-provider';
import { streamText } from 'ai';

export async function POST(req) {
  const { messages } = await req.json();

  const result = await streamText({
    model: adaptive(),
    messages,
  });

  return result.toDataStreamResponse();
}
```

```javascript API Route - OpenAI Provider
// pages/api/chat.js or app/api/chat/route.js
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const adaptiveOpenAI = openai({
  apiKey: process.env.ADAPTIVE_API_KEY,
  baseURL: 'https://www.llmadaptive.uk/api/v1',
});

export async function POST(req) {
  const { messages } = await req.json();

  const result = await streamText({
    model: adaptiveOpenAI(''),
    messages,
  });

  return result.toDataStreamResponse();
}
```
</CodeGroup>

---

## Configuration Parameters

<Info>
Advanced configuration options are available with the Adaptive provider for intelligent routing and optimization.
</Info>

<CodeGroup>
```javascript Intelligent Routing
import { generateText } from 'ai';
import { adaptive } from '@adaptive-llm/adaptive-ai-provider';

await generateText({
  model: adaptive(),
  prompt: "Summarize this article",
  providerOptions: {
    adaptive: {
      // Intelligent routing configuration
      model_router: {
        models: [
          { provider: "anthropic" }, // All Anthropic models
          { provider: "openai", model_name: "gpt-4" } // Specific OpenAI model
        ],
        cost_bias: 0.3, // 0 = cheapest, 1 = best performance
        complexity_threshold: 0.5, // Override complexity detection
        token_threshold: 1000 // Override token threshold
      }
    }
  }
});
```

```javascript Fallback Configuration
await generateText({
  model: adaptive(),
  prompt: "Explain machine learning",
  providerOptions: {
    adaptive: {
      // Fallback configuration
      fallback: {
        enabled: true, // Enable/disable fallback (default: true)
        mode: "race" // 'sequential' or 'race'
      }
    }
  }
});
```

```javascript Caching Options
await generateText({
  model: adaptive(),
  prompt: "What is TypeScript?",
  providerOptions: {
    adaptive: {
      // Semantic caching
      prompt_response_cache: {
        enabled: true,
        semantic_threshold: 0.85 // Similarity threshold for cache hits
      },
      
      // Prompt response cache for identical requests
      prompt_cache: {
        enabled: true,
        ttl: 3600 // Cache duration in seconds (default: 1 hour)
      }
    }
  }
});
```
</CodeGroup>

### Parameter Details

<Accordion title="model_router - Intelligent Model Selection">
Controls intelligent model selection:

- **`models`**: Array of allowed providers/models
  - `{ provider: "openai" }` - All models from provider
  - `{ provider: "anthropic", model_name: "claude-3-sonnet" }` - Specific model
- **`cost_bias`**: Balance cost vs performance (0-1)
  - `0` = Always choose cheapest option
  - `0.5` = Balanced cost and performance  
  - `1` = Always choose best performance
- **`complexity_threshold`**: Override automatic complexity detection (0-1)
- **`token_threshold`**: Override automatic token counting threshold
</Accordion>

<Accordion title="fallback - Provider Fallback Behavior">
Controls provider fallback behavior:

- **`enabled`**: Enable/disable fallback (default: true)
- **`mode`**: Fallback strategy
  - `"sequential"` = Try providers one by one (lower cost)
  - `"race"` = Try multiple providers simultaneously (faster)
</Accordion>

<Accordion title="prompt_response_cache - Semantic Caching">
Improves performance by caching similar requests:

- **`enabled`**: Enable semantic caching
- **`semantic_threshold`**: Similarity threshold (0-1) for cache hits
  - Higher values = more strict matching
  - Lower values = more cache hits but less accuracy
</Accordion>

<Accordion title="prompt_cache - Ultra-Fast Caching">
Ultra-fast caching for identical requests:

- **`enabled`**: Enable prompt response caching for this request
- **`ttl`**: Cache duration in seconds (default: 3600, i.e., 1 hour)
  - Provides sub-millisecond response times for repeated requests
  - Only successful responses are cached
</Accordion>

---

## Custom Providers

Configure custom providers alongside standard ones using the Adaptive provider:

```javascript Custom Provider Configuration
await generateText({
  model: adaptive(),
  prompt: "Explain machine learning concepts",
  providerOptions: {
    adaptive: {
      // Include custom provider in model list
      model_router: {
        models: [
          { provider: "openai" }, // Standard provider
          {
            provider: "my-custom-llm", // Custom provider
            model_name: "custom-model-v1",
            cost_per_1m_input_tokens: 2.0,
            cost_per_1m_output_tokens: 6.0,
            max_context_tokens: 16000,
            max_output_tokens: 4000,
            supports_function_calling: true,
            task_type: "Text Generation",
            complexity: "medium"
          }
        ],
        cost_bias: 0.5
      },
      
      // Configure each custom provider
      provider_configs: {
        "my-custom-llm": {
          base_url: "https://api.mycustom.com/v1",
          api_key: "sk-custom-api-key-here",
          auth_type: "bearer",
          headers: {
            "X-Custom-Header": "value"
          },
          timeout_ms: 45000
        }
      }
    }
  }
});
```

---

## Tool/Function Calling

<CodeGroup>
```javascript Adaptive Provider
import { generateText, tool } from 'ai';
import { z } from 'zod';
import { adaptive } from '@adaptive-llm/adaptive-ai-provider';

const { text } = await generateText({
  model: adaptive(),
  prompt: "What's the weather in New York?",
  tools: {
    getWeather: tool({
      description: 'Get weather for a location',
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => {
        return `Weather in ${location} is sunny and 72°F`;
      },
    }),
  },
});
```

```javascript OpenAI Provider
import { generateText, tool } from 'ai';
import { z } from 'zod';

const { text } = await generateText({
  model: adaptiveOpenAI(''),
  prompt: "What's the weather in New York?",
  tools: {
    getWeather: tool({
      description: 'Get weather for a location',
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => {
        return `Weather in ${location} is sunny and 72°F`;
      },
    }),
  },
});
```
</CodeGroup>

---

## Cache Tier Tracking

Access cache information in the response when using the Adaptive provider:

```javascript Cache Tier Information
const result = await generateText({
  model: adaptive(),
  prompt: "Hello world",
});

// Check cache tier
console.log(result.usage?.cache_tier);
// "semantic_exact" | "semantic_similar" | "prompt_response" | undefined
```

---

## Environment Variables

```bash Environment Setup
# .env.local
ADAPTIVE_API_KEY=your-adaptive-api-key
```

---

## What You Get

<CardGroup cols={2}>
  <Card title="Intelligent Routing" icon="brain">
    Automatic model selection based on your prompt complexity
  </Card>
  <Card title="Built-in Streaming" icon="zap">
    Real-time response streaming with React components
  </Card>
  <Card title="Cost Optimization" icon="dollar-sign">
    Significant cost savings through smart provider selection
  </Card>
  <Card title="Provider Transparency" icon="eye">
    See which AI provider was used for each request
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="More Examples" href="/examples/streaming-chat" icon="code">
    See complete working examples
  </Card>
  <Card title="API Reference" href="/api-reference/chat-completions" icon="book">
    Explore all available options and parameters
  </Card>
</CardGroup>