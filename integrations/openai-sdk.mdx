---
title: 'OpenAI SDK'
description: 'Drop-in replacement for OpenAI with intelligent routing'
icon: "/images/logos/openai.svg"
---

## Quick Setup

1. [Sign up for an Adaptive API key](https://www.llmadaptive.uk/sign-up?redirect_url=/api-platform/orgs)

2. **Install the OpenAI SDK** (if you haven't already):

<CodeGroup>
```bash npm
npm install openai
```

```bash yarn  
yarn add openai
```

```bash pnpm
pnpm add openai
```
</CodeGroup>

2. **Change two lines** in your existing code:

<CodeGroup>
```javascript JavaScript/Node.js
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'your-adaptive-api-key',        // ← Your Adaptive API key
  baseURL: 'https://api.llmadaptive.uk/v1' // ← Only change needed
});

// Everything else works exactly the same
const completion = await openai.chat.completions.create({
  model: '', // Leave empty for intelligent routing
  messages: [{ role: 'user', content: 'Hello!' }]
});
```

```python Python
from openai import OpenAI

client = OpenAI(
    api_key="your-adaptive-api-key",        # ← Your Adaptive API key  
    base_url="https://api.llmadaptive.uk/v1" # ← Only change needed
)

# Everything else works exactly the same
completion = client.chat.completions.create(
    model="", # Leave empty for intelligent routing
    messages=[{"role": "user", "content": "Hello!"}]
)
```
</CodeGroup>

That's it! Your existing OpenAI code now uses intelligent routing.

## Key Features

<CardGroup cols={2}>
  <Card title="Same API" icon="check">
    All OpenAI methods, parameters, and responses work identically
  </Card>
  <Card title="Intelligent Routing" icon="brain">
    Leave model empty ("") to automatically select the best provider
  </Card>
  <Card title="Streaming" icon="zap">
    Streaming responses work exactly like OpenAI
  </Card>
  <Card title="Function Calling" icon="function">
    Function calling and tools work without changes
  </Card>
</CardGroup>

## Streaming Example

<CodeGroup>
```javascript JavaScript
const stream = await openai.chat.completions.create({
  model: '',
  messages: [{ role: 'user', content: 'Tell me a story' }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

```python Python
stream = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```
</CodeGroup>

## Response Format

Adaptive returns standard OpenAI responses with one extra field:

```json
{
  "id": "chatcmpl-123",
  "choices": [{
    "message": {"content": "Hello! How can I help you?"}
  }],
  "usage": {"total_tokens": 21},
  "provider": "gemini"  // ← Shows which provider was used
}
```

## Error Handling

Adaptive automatically handles provider failures and retries. For comprehensive error handling patterns, see the [Error Handling Guide](/guides/error-handling).

```typescript
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "your-adaptive-api-key",
  baseURL: "https://api.llmadaptive.uk/v1",
});

try {
  const response = await client.chat.completions.create({
    model: "", // Intelligent routing with automatic retries
    messages: [{ role: "user", content: "Hello!" }],
  });
  console.log(response.choices[0].message.content);
} catch (error: any) {
  console.error("Request failed:", error.message);
  // Adaptive has already tried multiple providers
}
```

### Streaming Error Recovery

```typescript
async function streamWithErrorRecovery(messages: any[]) {
  try {
    const stream = await client.chat.completions.create({
      model: "",
      messages,
      stream: true,
    });

    let buffer = "";
    let errorCount = 0;
    const maxErrors = 3;

    for await (const chunk of stream) {
      try {
        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          buffer += content;
          process.stdout.write(content);
        }

        // Reset error count on successful chunk
        errorCount = 0;

      } catch (chunkError) {
        errorCount++;
        console.warn(`Chunk processing error ${errorCount}:`, chunkError);

        if (errorCount >= maxErrors) {
          console.error("Too many streaming errors, aborting...");
          break;
        }

        // Continue processing other chunks
        continue;
      }
    }

    return buffer;

  } catch (streamError: any) {
    console.error("Streaming failed:", streamError);

    // Fallback to non-streaming request
    console.log("Falling back to non-streaming request...");
    return await client.chat.completions.create({
      model: "",
      messages,
      stream: false,
    });
  }
}
```

### Circuit Breaker Pattern

```typescript
class CircuitBreaker {
  private failures = 0;
  private lastFailureTime = 0;
  private state: 'closed' | 'open' | 'half-open' = 'closed';

  constructor(
    private failureThreshold = 5,
    private recoveryTimeout = 60000 // 1 minute
  ) {}

  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailureTime > this.recoveryTimeout) {
        this.state = 'half-open';
      } else {
        throw new Error('Circuit breaker is open');
      }
    }

    try {
      const result = await operation();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess() {
    this.failures = 0;
    this.state = 'closed';
  }

  private onFailure() {
    this.failures++;
    this.lastFailureTime = Date.now();

    if (this.failures >= this.failureThreshold) {
      this.state = 'open';
    }
  }
}

// Usage
const circuitBreaker = new CircuitBreaker();

async function safeApiCall(messages: any[]) {
  return circuitBreaker.execute(async () => {
    return await client.chat.completions.create({
      model: "",
      messages,
    });
  });
}
```

### Batch Processing with Error Isolation

```typescript
async function processBatchWithIsolation(items: any[], batchSize = 10) {
  const results = [];
  const errors = [];

  for (let i = 0; i < items.length; i += batchSize) {
    const batch = items.slice(i, i + batchSize);
    const batchPromises = batch.map(async (item, index) => {
      try {
        const response = await client.chat.completions.create({
          model: "",
          messages: [{ role: "user", content: item.prompt }],
        });
        return {
          index: i + index,
          success: true,
          result: response.choices[0].message.content,
        };
      } catch (error: any) {
        return {
          index: i + index,
          success: false,
          error: error.message,
          item,
        };
      }
    });

    const batchResults = await Promise.allSettled(batchPromises);

    batchResults.forEach((result) => {
      if (result.status === 'fulfilled') {
        if (result.value.success) {
          results.push(result.value);
        } else {
          errors.push(result.value);
        }
      } else {
        errors.push({
          index: -1,
          success: false,
          error: result.reason.message,
        });
      }
    });
  }

  return { results, errors };
}

// Usage
const items = [
  { prompt: "Summarize AI trends" },
  { prompt: "Explain machine learning" },
  // ... more items
];

const { results, errors } = await processBatchWithIsolation(items);

console.log(`Processed ${results.length} items successfully`);
console.log(`Encountered ${errors.length} errors`);
```

---

## Need More Control?

<CardGroup cols={2}>
  <Card title="API Reference" href="/api-reference/chat-completions" icon="book">
    See all available parameters and options
  </Card>
  <Card title="Examples" href="/examples/basic-chat" icon="code">
    Working code examples and use cases
  </Card>
</CardGroup>