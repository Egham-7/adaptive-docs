---
title: "Prompt Caching"
description: "Multi-layer caching combining semantic understanding with provider-level optimization"
icon: "brain"
---

Adaptive implements multi-layer prompt caching combining AI-powered semantic understanding with provider-level infrastructure caching. This automatically optimizes for both similar queries and exact repeats, reducing costs by 60-95%.

<Note>
  **Automatic**: Both semantic and provider caching work without configuration for most applications.
</Note>

## How It Works

Adaptive checks caches in order of speed:

1. **Provider Cache**: Exact matches at provider infrastructure (microseconds)
2. **Semantic Cache**: AI-powered similar meaning matches (&lt;100ms)
3. **Fresh Request**: Standard API call when no cache hit

## Key Benefits

<CardGroup cols={4}>
  <Card title="Speed" icon="zap">
    **Microseconds - 100ms** across cache levels
  </Card>
  <Card title="Savings" icon="dollar-sign">
    **60-95%** cost reduction
  </Card>
  <Card title="Intelligence" icon="brain">
    **Exact + semantic** matching
  </Card>
  <Card title="Integration" icon="server">
    **Provider native** caching
  </Card>
</CardGroup>

## Real-World Examples

**Customer Support**: "How do I reset my password?" matches "I forgot my password", "Password reset steps", "Help me recover my account"

**Technical Docs**: "How to implement JWT authentication" matches "JWT auth implementation guide", "Setting up JSON Web Token auth"

**FAQ Systems**: "What are your business hours?" matches "When are you open?", "Office hours information"

## Provider Prompt Caching

Adaptive automatically leverages provider-level caching when available:

| Provider | Cache Reads | Configuration | Notes |
|----------|-------------|---------------|-------|
| **OpenAI** | 0.25x-0.50x cost | Automatic | 1024+ tokens minimum |
| **Anthropic** | 0.1x cost | `cache_control` required | 4 breakpoints max, 5min TTL |
| **Google Gemini** | 0.25x cost | Automatic or explicit | 1028-2048 tokens minimum |
| **DeepSeek** | 0.1x cost | Automatic | - |
| **Grok** | 0.25x cost | Automatic | - |
| **Groq** | Reduced rate | Automatic | Kimi K2 models |

<CodeGroup>
```javascript Anthropic Cache Control
const message = await client.messages.create({
  model: 'claude-sonnet-4-5',
  messages: [{
    role: 'user',
    content: [
      { type: 'text', text: 'Based on this document:' },
      { type: 'text', text: 'LARGE TEXT CONTENT HERE', cache_control: { type: 'ephemeral' } },
      { type: 'text', text: 'Summarize the key points.' }
    ]
  }]
});
```

```javascript Gemini Cache Control
const result = await model.generateContent({
  contents: [{
    role: 'user',
    parts: [
      { text: 'Analyze this data:' },
      { text: 'LARGE TEXT CONTENT HERE', cache_control: { type: 'ephemeral' } },
      { text: 'What are the main trends?' }
    ]
  }]
});
```
</CodeGroup>

### Provider vs Semantic Cache

| Type | Matching | Speed | Savings |
|------|----------|-------|---------|
| **Provider** | Exact prompts only | Microseconds | 90-95% |
| **Semantic** | Similar meaning | &lt;100ms | 60-90% |
| **Combined** | Best available | Automatic | Maximum |

### Monitoring Provider Cache Hits

Provider cache hits are indicated differently than semantic cache hits:

```javascript Provider Cache Detection
const completion = await client.chat.completions.create({
  model: '',
  messages: [{ role: 'user', content: 'Hello!' }]
});

// Provider cache hits may show:
// - Extremely fast response times (<50ms)
// - Lower token counts (reused context)
console.log(`Response time: ${Date.now() - startTime}ms`);
console.log(`Cache tier: ${completion.usage.cache_tier || 'none'}`);
console.log(`Provider: ${completion.provider}`);
```

### Provider Cache Best Practices

<Warning>
**Provider caching is automatic** but can be optimized with consistent prompt patterns.
</Warning>

<Steps>
  <Step title="Use Consistent System Prompts">
    Keep system messages identical across similar requests for provider cache hits
  </Step>
  <Step title="Maintain Conversation Context">
    Provider caches work best with ongoing conversations using the same context
  </Step>
  <Step title="Monitor Response Times">
    Very fast responses (&lt;50ms) often indicate provider cache hits
  </Step>
  <Step title="Combine with Semantic Cache">
    Let Adaptive automatically choose the best caching strategy
  </Step>
</Steps>

## SDK Setup

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.ADAPTIVE_API_KEY,
  baseURL: 'https://api.llmadaptive.uk/v1'
});
```

## Configuration

**Default**: Semantic cache enabled automatically - no configuration needed.

**Custom Threshold**:
```javascript
// Stricter matching
model_router: { semantic_cache: { threshold: 0.9 } }

// Looser matching  
model_router: { semantic_cache: { threshold: 0.75 } }

// Disable for real-time data
model_router: { semantic_cache: { enabled: false } }
```

### Configuration Parameters

- `enabled`: Enable/disable semantic caching (default: true)
- `threshold`: Similarity threshold 0.0-1.0 (default: 0.85)

## Threshold Guide

- **0.7-0.8**: Loose matching for FAQ/customer support (high hit rate)
- **0.8-0.9**: Balanced (default) for most applications
- **0.9+**: Strict matching for technical/legal content (high precision)

## Cache Performance Tracking

Every response includes cache information:

```json
{
  "usage": {
    "cache_tier": "semantic_similar",
    "total_tokens": 40
  },
  "provider": "cached"
}
```

### Cache Tier Values

<CardGroup cols={3}>
  <Card title="semantic_exact" icon="bullseye">
    **Perfect semantic match**: 90%+ savings, &lt;50ms response
  </Card>
  <Card title="semantic_similar" icon="target">
    **Similar semantic match**: 60-90% savings, &lt;100ms response
  </Card>
  <Card title="provider_cache" icon="server">
    **Provider infrastructure cache**: Microsecond response, high savings
  </Card>
  <Card title="none" icon="x">
    **No cache hit**: Standard latency and costs
  </Card>
</CardGroup>

## Performance Characteristics

**Performance**: Microseconds-100ms latency, 60-80% hit rate, 60-95% cost savings

## Use Cases

**Perfect for**: Customer support, FAQ systems, documentation search, educational content

**Avoid for**: Real-time data, personalized content, time-sensitive information

## Best Practices

- Use default settings (0.85 threshold) for most applications
- Monitor `cache_tier` in responses to track effectiveness
- Lower thresholds for FAQ systems, higher for technical content
- Group similar content to maximize cache hits

## Error Handling

Cache failures automatically fallback to fresh API calls. No interruption to your requests.

## Monitoring

Track cache performance in your Adaptive dashboard: hit rates, cost savings, and response times.

## Technical Details

- **Multi-layer**: Provider infrastructure + AI semantic caching
- **Embeddings**: Sentence transformers for semantic matching
- **Similarity**: Cosine similarity with configurable thresholds
- **Storage**: Dual systems with automatic fallback

## Next Steps

- [Performance Overview](/features/performance) - All optimization features
- [Model Routing](/features/model-routing) - Routing + caching
