---
title: "Prompt Response Cache"
description: "Ultra-fast caching for identical requests with semantic matching"
icon: "clock"
---

The prompt response cache delivers sub-millisecond response times by storing complete responses for requests and using AI-powered semantic matching to find similar prompts. This cache layer provides the fastest possible responses when enabled.

<Note>
**Important**: Prompt response caching is disabled by default. Enable it per request when you want maximum speed for repeated or similar queries.
</Note>

## How It Works

<Steps>
  <Step title="Semantic Analysis">
    Incoming prompts are converted to AI embeddings to understand meaning and context
  </Step>
  <Step title="Intelligent Matching">
    The system looks for both exact matches and semantically similar cached responses  
  </Step>
  <Step title="Instant Delivery">
    Cached responses are returned in under 100ms, converted to your preferred format
  </Step>
  <Step title="Streaming Simulation">
    Cached responses work with streaming by chunking content with realistic timing
  </Step>
</Steps>

## Key Benefits

<CardGroup cols={4}>
  <Card title="Speed" icon="zap">
    **&lt;100ms**  
    Response times
  </Card>
  <Card title="Cost" icon="dollar-sign">
    **$0.00**  
    Zero API costs
  </Card>
  <Card title="Accuracy" icon="target">
    **100%**  
    Identical responses
  </Card>
  <Card title="Reliability" icon="shield-check">
    **Redis-backed**  
    Persistent storage
  </Card>
</CardGroup>

## Quick Start

Enable prompt caching by adding the `prompt_cache` parameter:

<CodeGroup>
```javascript JavaScript/Node.js
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "What is 2+2?" }],
  temperature: 0.7,
  prompt_cache: {
    enabled: true,
    ttl: 3600 // 1 hour (optional)
  }
});

// Check if response came from cache
console.log(`Cache tier: ${completion.usage.cache_tier}`);
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "What is 2+2?"}],
    temperature=0.7,
    prompt_cache={
        "enabled": True,
        "ttl": 3600  # 1 hour (optional)
    }
)

# Check if response came from cache
print(f"Cache tier: {completion.usage.cache_tier}")
```

```bash cURL
curl https://www.llmadaptive.uk/api/v1/chat/completions \
  -H "Authorization: Bearer your-adaptive-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "temperature": 0.7,
    "prompt_cache": {
      "enabled": true,
      "ttl": 3600
    }
  }'
```
</CodeGroup>

## Cache Behavior

### What Gets Cached

<CardGroup cols={2}>
  <Card title="Semantic Content" icon="brain">
    **Prompt meaning**: AI understands different phrasings of the same question
  </Card>
  <Card title="Exact Parameters" icon="sliders">
    **Request settings**: temperature, max_tokens, and other parameters must match exactly
  </Card>
  <Card title="Successful Responses" icon="check-circle">
    **Only success**: Error responses are never cached to prevent propagation
  </Card>
  <Card title="Model Compatibility" icon="arrows-rotate">
    **Cross-model**: Works across compatible models when semantically similar
  </Card>
</CardGroup>

### Cache Matching Examples

<Tabs>
<Tab title="Semantic Matches">
These prompts would all hit the same cache entry:

```javascript
// Original request
"What is 2+2?"

// Semantic matches (cache hits)
"Tell me 2 plus 2"
"What's two plus two?"  
"Calculate 2 + 2"
"What does 2+2 equal?"
```

All return the same cached response with `cache_tier: "semantic_similar"`
</Tab>

<Tab title="Parameter Requirements">
For a cache hit, these parameters must match exactly:

```javascript
// Cache hit - same parameters
{ temperature: 0.7, max_tokens: 100, top_p: 1.0 }

// Cache miss - different temperature  
{ temperature: 0.8, max_tokens: 100, top_p: 1.0 }

// Cache miss - different max_tokens
{ temperature: 0.7, max_tokens: 150, top_p: 1.0 }
```
</Tab>
</Tabs>

## Cache Priority System

Adaptive checks caches in priority order for optimal performance:

<Steps>
  <Step title="L1: Prompt Response Cache">
    **Fastest**: Exact matches return in microseconds
  </Step>
  <Step title="L2: Semantic Cache">  
    **Fast**: Similar meaning matches in 1-2ms
  </Step>
  <Step title="L3: Fresh Request">
    **Standard**: New API call to provider
  </Step>
</Steps>

## Configuration Options

<ParamField body="prompt_cache" type="object" required>
  Configuration for prompt response caching
  
  <Expandable title="Properties">
    <ParamField body="enabled" type="boolean" required>
      Must be `true` to enable caching for this request
    </ParamField>
    
    <ParamField body="ttl" type="integer">
      Cache duration in seconds (default: 3600 = 1 hour)
    </ParamField>
  </Expandable>
</ParamField>

### TTL (Time To Live) Guidelines

<CardGroup cols={3}>
  <Card title="Static Content" icon="file-text">
    **3600-86400s** (1-24 hours)  
    Documentation, facts, definitions
  </Card>
  <Card title="Semi-Dynamic" icon="clock">
    **600-3600s** (10 minutes - 1 hour)  
    Analysis, explanations, tutorials
  </Card>
  <Card title="Dynamic Content" icon="refresh">
    **60-600s** (1-10 minutes)  
    Time-sensitive information
  </Card>
</CardGroup>

## Streaming Support

Cached responses work seamlessly with streaming requests:

<CodeGroup>
```javascript Streaming Example
const stream = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Write a poem about AI" }],
  stream: true,
  prompt_cache: {
    enabled: true,
    ttl: 3600
  }
});

// Cached responses stream naturally with 10ms delays between chunks
for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

```python Streaming Example
stream = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Write a poem about AI"}],
    stream=True,
    prompt_cache={
        "enabled": True,
        "ttl": 3600
    }
)

# Cached responses stream naturally with realistic timing
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
```
</CodeGroup>

## Cache Performance Tracking

### Response Metadata

Every cached response includes performance information:

```json
{
  "id": "chatcmpl-abc123",
  "choices": [{"message": {"content": "2+2 equals 4"}}],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30,
    "cache_tier": "semantic_exact"  // Cache performance indicator
  },
  "provider": "cached",
  "model": "cached-response"
}
```

### Cache Tier Values

<CardGroup cols={2}>
  <Card title="semantic_exact" icon="bullseye">
    **Perfect match**: Identical prompt and parameters found
  </Card>
  <Card title="semantic_similar" icon="target">
    **Similar match**: Semantically equivalent prompt found  
  </Card>
  <Card title="undefined" icon="x">
    **No cache**: Fresh response from API provider
  </Card>
  <Card title="prompt_response" icon="database">
    **Legacy**: Old cache format (being phased out)
  </Card>
</CardGroup>

## Performance Characteristics

<CardGroup cols={4}>
  <Card title="Hit Latency" icon="zap">
    **50-100ms**  
    Including semantic analysis
  </Card>
  <Card title="Miss Overhead" icon="plus">
    **10-20ms**  
    Added for embedding generation
  </Card>
  <Card title="Hit Rate" icon="percent">
    **25-40%**  
    Typical applications
  </Card>
  <Card title="Storage" icon="database">
    **Redis**  
    Persistent, scalable
  </Card>
</CardGroup>

## Best Practices

### For Maximum Effectiveness

<Steps>
  <Step title="Use Deterministic Settings">
    Set `temperature: 0` for consistent, cacheable results
  </Step>
  <Step title="Choose Appropriate TTL">
    Match cache duration to your content's freshness requirements
  </Step>
  <Step title="Monitor Performance">
    Track `cache_tier` values to measure cache effectiveness
  </Step>
  <Step title="Consider Memory Usage">
    Monitor Redis usage, especially with high TTL values
  </Step>
</Steps>

### Ideal Use Cases

<CardGroup cols={2}>
  <Card title="FAQ Systems" icon="question-circle">
    **Perfect fit**: Users ask similar questions in different ways
  </Card>
  <Card title="Documentation Queries" icon="book">
    **High hit rate**: Repeated searches for the same information
  </Card>
  <Card title="Educational Content" icon="graduation-cap">
    **Consistent responses**: Same explanations for similar concepts
  </Card>
  <Card title="API Responses" icon="code">
    **Fast lookups**: Repeated queries for similar data
  </Card>
</CardGroup>

## Error Handling

<Warning>
**Graceful Degradation**: Cache failures never block your requests - they automatically fallback to fresh API calls.
</Warning>

### Failure Scenarios

<CardGroup cols={2}>
  <Card title="Redis Issues" icon="server">
    **Automatic fallback** to fresh API requests with no user impact
  </Card>
  <Card title="Cache Corruption" icon="exclamation-triangle">
    **Self-healing** - corrupted entries are automatically removed
  </Card>
  <Card title="Embedding Errors" icon="brain">
    **Skip semantic matching** and proceed with standard routing
  </Card>
  <Card title="Memory Limits" icon="memory">
    **LRU eviction** - oldest entries removed to make space
  </Card>
</CardGroup>

## Monitoring and Analytics

Track cache performance in your [Adaptive dashboard](https://www.llmadaptive.uk/dashboard):

- Cache hit rates and trends
- Semantic matching accuracy
- Cost savings from cached responses  
- Response time improvements
- Memory usage and capacity planning

## Next Steps

<CardGroup cols={2}>
  <Card title="Semantic Cache" href="/features/semantic-cache" icon="database">
    Learn about the broader semantic caching system
  </Card>
  <Card title="Performance Overview" href="/features/performance" icon="gauge-high">
    Understand all performance optimization features
  </Card>
</CardGroup>
