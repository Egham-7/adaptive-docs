---
title: "Performance"
description: "Ultra-fast routing with multi-tier caching and optimized Go architecture"
icon: "gauge-high"
---

Adaptive delivers industry-leading performance through optimized Go architecture, intelligent caching, and purpose-built ML algorithms designed for speed.

## Performance Highlights

<CardGroup cols={4}>
  <Card title="Model Selection" icon="zap">
    **&lt;1ms**  
    Instant routing decisions
  </Card>
  <Card title="Throughput" icon="chart-line">
    **10,000+**  
    Requests per second
  </Card>
  <Card title="Cache Hit Rate" icon="database">
    **60-90%**
    Multi-tier efficiency
  </Card>
  <Card title="Overhead" icon="clock">
    **&lt;3ms**  
    Total added latency
  </Card>
</CardGroup>

## Architecture Advantages

### Lightning-Fast ML Pipeline

<Steps>
  <Step title="No LLMs in Routing">
    Pure ML classifiers make decisions instantly without large model inference
  </Step>
  <Step title="Pre-computed Embeddings">
    Classification happens without real-time embedding generation
  </Step>
  <Step title="Optimized Algorithms">
    Purpose-built for speed over complexity with zero unnecessary overhead
  </Step>
</Steps>

### Multi-Tier Caching System

**L1: Prompt-Response Cache** - Microsecond responses for identical requests (40-60% hit rate)  
**L2: Semantic Cache** - 1-2ms responses for similar meaning requests (20-30% hit rate)  
**L3: Router Cache** - 5-10ms responses for provider health decisions (nearly 100% hit rate)

### Go-Powered Backend

<CardGroup cols={2}>
  <Card title="Native Performance" icon="rocket">
    Compiled binary with no runtime overhead
  </Card>
  <Card title="Massive Concurrency" icon="users">
    Handle thousands of simultaneous requests with goroutines
  </Card>
  <Card title="Memory Efficient" icon="cpu">
    Minimal garbage collection impact
  </Card>
  <Card title="Fast Startup" icon="play">
    Sub-second cold start times
  </Card>
</CardGroup>

## Real-World Metrics

### Latency Breakdown

```text
┌─────────────────┬──────────────┐
│ Component       │ Time         │
├─────────────────┼──────────────┤
│ Model Selection │ &lt;1ms         │
│ Cache Lookup    │ &lt;1ms         │
│ Provider Route  │ &lt;1ms         │
├─────────────────┼──────────────┤
│ Total Overhead  │ &lt;3ms         │
└─────────────────┴──────────────┘
```

### Throughput Characteristics

<CardGroup cols={3}>
  <Card title="Sustained Load" icon="server">
    **10,000+ req/s**  
    Continuous high throughput
  </Card>
  <Card title="Burst Capacity" icon="trending-up">
    **50,000+ req/s**  
    Short-term peak handling
  </Card>
  <Card title="Linear Scaling" icon="expand">
    **2x instances = 2x capacity**  
    Predictable performance scaling
  </Card>
</CardGroup>

## Performance Optimizations

### Smart Request Processing

<Steps>
  <Step title="Parallel Classification">
    Request analysis happens concurrently with provider health checks
  </Step>
  <Step title="Pre-computed Routes">
    Common routing decisions are cached and reused across requests
  </Step>
  <Step title="Async Health Checks">
    Provider status updates happen in background without blocking requests
  </Step>
</Steps>

### Efficient Data Handling

<CardGroup cols={2}>
  <Card title="Zero-Copy Operations" icon="copy">
    Minimal memory allocation and copying during request processing
  </Card>
  <Card title="Connection Pooling" icon="network-wired">
    Persistent connections to all providers reduce connection overhead
  </Card>
  <Card title="Optimized JSON" icon="code">
    Fast parsing and serialization with minimal allocations
  </Card>
  <Card title="Resource Management" icon="gauge">
    Graceful degradation under extreme load conditions
  </Card>
</CardGroup>

## Performance Comparison

<Note>
Benchmarks run on identical hardware (4 CPU cores, 8GB RAM) with 1000 concurrent requests.
</Note>

| Solution | Model Selection | Memory Usage | Cold Start | Throughput |
|----------|----------------|--------------|------------|------------|
| **Adaptive** | &lt;1ms | 50MB | &lt;1s | 10,000+ req/s |
| Python-based | 50-200ms | 500MB+ | 10-30s | 500 req/s |
| LLM-based routing | 1-5s | 2GB+ | 60s+ | 10 req/s |

## Cache Performance

### Hit Rate Optimization

<CardGroup cols={3}>
  <Card title="Repeated Queries" icon="repeat">
    **90%+ hit rate**  
    FAQ-style applications
  </Card>
  <Card title="Similar Content" icon="clone">
    **60-70% hit rate**  
    Content generation tasks
  </Card>
  <Card title="Unique Requests" icon="snowflake">
    **20-30% hit rate**  
    Highly varied applications
  </Card>
</CardGroup>

### Cache Warming

Adaptive automatically pre-loads cache with common patterns. Manual warming:

```javascript
const patterns = ["Hello, how are you?", "Explain this concept:", "Write a summary of:"];
for (const pattern of patterns) {
  await openai.chat.completions.create({
    model: "",
    messages: [{ role: "user", content: pattern }]
  });
}
```

## Monitoring and Observability

Track performance in your [Adaptive dashboard](https://www.llmadaptive.uk/dashboard):

- Request latency trends and percentiles
- Cache hit rates across all tiers
- Provider performance comparisons
- Cost savings from cache hits
- Throughput and scaling metrics

<Warning>
**Performance Tip**: Enable semantic caching for applications with similar but not identical requests to maximize cache efficiency.
</Warning>

## Scaling Considerations

### Horizontal Scaling

<Steps>
  <Step title="Load Balancing">
    Multiple Adaptive instances can be load-balanced for higher throughput
  </Step>
  <Step title="Cache Sharing">
    Distributed cache layers maintain efficiency across instances
  </Step>
  <Step title="Auto-scaling">
    Automatic instance scaling based on request volume and latency
  </Step>
</Steps>

### Performance Best Practices

<CardGroup cols={2}>
  <Card title="Enable All Caches" icon="toggle-on">
    Use both semantic and prompt-response caching for maximum performance
  </Card>
  <Card title="Connection Reuse" icon="recycle">
    Use persistent connections and connection pooling in your clients
  </Card>
  <Card title="Batch Requests" icon="layer-group">
    Group similar requests together when possible for better cache efficiency
  </Card>
  <Card title="Monitor Metrics" icon="chart-mixed">
    Watch performance dashboards to identify optimization opportunities
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={1}>
  <Card title="Prompt Caching" href="/features/prompt-cache" icon="database">
    Learn about intelligent content-aware caching
  </Card>
</CardGroup>
