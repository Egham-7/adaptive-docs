---
title: "Intelligent Routing"
description: "Automatic model selection for optimal cost and performance"
icon: "route"
---

Adaptive's AI-powered routing engine analyzes every request and automatically selects the optimal model from multiple providers based on complexity, cost, and performance requirements.

## How It Works

<Steps>
  <Step title="Request Analysis">
    Our ML models analyze your prompt's complexity, length, task type, and function calling requirements in real-time
  </Step>
  <Step title="Provider Selection">
    The routing engine considers available providers, costs, performance metrics, and function calling support
  </Step>
  <Step title="Optimal Match">
    The best model is selected and your request is routed automatically
  </Step>
  <Step title="Response Delivery">
    You receive a standard response with provider information showing which model was used
  </Step>
</Steps>

## Quick Start

Simply leave the model field empty to enable intelligent routing:

<CodeGroup>
```javascript JavaScript/Node.js
const completion = await openai.chat.completions.create({
  model: "", // Empty enables intelligent routing
  messages: [{ role: "user", content: "Hello!" }]
});

console.log(`Used provider: ${completion.provider}`);
```

```python Python
completion = client.chat.completions.create(
    model="", # Empty enables intelligent routing
    messages=[{"role": "user", "content": "Hello!"}]
)

print(f"Used provider: {completion.provider}")
```

```bash cURL
curl https://www.llmadaptive.uk/api/v1/chat/completions \
  -H "Authorization: Bearer your-adaptive-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
</CodeGroup>

## Real Examples

<CardGroup cols={2}>
  <Card title="Simple Greeting" icon="message">
    **"Hello, how are you?"**
    
    Routes to: Gemini Flash  
    Cost: **$0.10** per 1M tokens  
    Savings: **97%** vs GPT-4
  </Card>
  <Card title="Code Generation" icon="code">
    **"Write a React component..."**
    
    Routes to: DeepSeek Coder  
    Cost: **$0.34** per 1M tokens  
    Savings: **87%** vs GPT-4
  </Card>
  <Card title="Complex Analysis" icon="brain">
    **"Analyze this dataset..."**
    
    Routes to: Claude Sonnet  
    Cost: **$2.19** per 1M tokens  
    Savings: **72%** vs GPT-4
  </Card>
  <Card title="Function Calling" icon="tools">
    **"What's the weather?" + tools**
    
    Routes to: GPT-4o Mini  
    Prioritizes function calling support  
    Smart tool-capable model selection
  </Card>
</CardGroup>

## Configuration Options

### Function Calling Support

When tools are provided, Adaptive automatically prioritizes models with function calling capabilities:

<CodeGroup>
```javascript JavaScript/Node.js
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "What's the weather in San Francisco?" }],
  tools: [{
    type: "function",
    function: {
      name: "get_weather",
      description: "Get current weather for a location",
      parameters: {
        type: "object",
        properties: {
          location: { type: "string", description: "City name" }
        },
        required: ["location"]
      }
    }
  }]
});

// Automatically routes to models that support function calling
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "What's the weather in San Francisco?"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name"}
                },
                "required": ["location"]
            }
        }
    }]
)

# Automatically routes to models that support function calling
```
</CodeGroup>

### Control Cost vs Performance

Balance between cost savings and response quality:

<CodeGroup>
```javascript JavaScript/Node.js
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Explain quantum physics" }],
  cost_bias: 0.3 // 0 = cheapest, 0.5 = balanced, 1 = best performance
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Explain quantum physics"}],
    cost_bias=0.3  # 0 = cheapest, 0.5 = balanced, 1 = best performance
)
```
</CodeGroup>

### Limit Available Providers

Restrict routing to specific providers or models:

<CodeGroup>
```javascript JavaScript/Node.js
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Write a story" }],
  models: [
    { provider: "openai" }, // All OpenAI models
    { provider: "anthropic", model_name: "claude-3-sonnet" } // Specific model
  ]
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Write a story"}],
    models=[
        {"provider": "openai"},  # All OpenAI models
        {"provider": "anthropic", "model_name": "claude-3-sonnet"}  # Specific model
    ]
)
```
</CodeGroup>

## Routing Performance

<CardGroup cols={3}>
  <Card title="Accuracy" icon="target">
    **94%** accurate model selection based on prompt analysis
  </Card>
  <Card title="Speed" icon="zap">
    **&lt;1ms** routing decision time with zero added latency
  </Card>
  <Card title="Reliability" icon="shield-check">
    **99.9%** uptime with automatic failover mechanisms
  </Card>
</CardGroup>

## Preview Routing Decisions

Want to see which model would be selected before making the request? Use our model selection preview:

<CodeGroup>
```javascript JavaScript/Node.js
// Preview which model would be selected
const response = await fetch('https://www.llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 
    'Authorization': 'Bearer your-adaptive-key',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    prompt: 'Complex data analysis task',
    models: [
      { provider: 'openai' },
      { provider: 'anthropic' },
      { provider: 'deepseek' }
    ],
    cost_bias: 0.5
  })
});

const result = await response.json();
console.log(`Would select: ${result.provider}/${result.model}`);
console.log(`Alternatives: ${JSON.stringify(result.alternatives)}`);
```

```python Python
import requests

# Preview which model would be selected
response = requests.post(
    'https://www.llmadaptive.uk/api/v1/select-model',
    headers={'Authorization': 'Bearer your-adaptive-key'},
    json={
        'prompt': 'Complex data analysis task',
        'models': [
            {'provider': 'openai'},
            {'provider': 'anthropic'},
            {'provider': 'deepseek'}
        ],
        'cost_bias': 0.5
    }
)

result = response.json()
print(f"Would select: {result['provider']}/{result['model']}")
print(f"Alternatives: {result.get('alternatives', [])}")
```
</CodeGroup>

## Response Information

Every response includes provider information:

```json
{
  "id": "chatcmpl-abc123",
  "choices": [{
    "message": {"content": "Hello! How can I help you today?"}
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  },
  "provider": "gemini",     // Which provider was selected
  "model": "gemini-flash"   // Specific model used
}
```

## Advanced Use Cases

<CardGroup cols={2}>
  <Card title="Enterprise Optimization" icon="building">
    **Custom provider contracts**: Use intelligent routing with your own API keys and enterprise pricing
  </Card>
  <Card title="Local Deployment" icon="server">
    **On-premise inference**: Get cloud-quality routing decisions for local model deployments
  </Card>
  <Card title="A/B Testing" icon="flask">
    **Model comparison**: Preview different routing strategies before implementing them
  </Card>
  <Card title="Cost Monitoring" icon="chart-line">
    **Budget control**: Set cost thresholds and optimize spending automatically
  </Card>
</CardGroup>

## Best Practices

<Note>
**Tip**: Start with `cost_bias: 0.3` for most applications. This provides excellent cost savings while maintaining high quality responses.
</Note>

<Warning>
**Important**: Always handle the case where no suitable model is found. The API will return an error with suggested alternatives.
</Warning>

## Next Steps

<CardGroup cols={2}>
  <Card title="Performance Features" href="/features/performance" icon="zap">
    Learn about caching and performance optimizations
  </Card>
  <Card title="Provider Resiliency" href="/features/provider-resiliency" icon="shield">
    Understand our failover and reliability features
  </Card>
</CardGroup>